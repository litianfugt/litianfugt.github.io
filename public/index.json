[{"content":"今天对博客进行了一些优化，增加了这个\u0026quot;随想\u0026quot;页面，希望可以更方便地记录日常的想法和感悟。\n最近逐渐完善这个博客，希望后续自己能够多总结，多反思，拓展自己知识的广度和深度；\n","permalink":"http://localhost:13141/thoughts/2025-10-24-first-thought/","summary":"\u003cp\u003e今天对博客进行了一些优化，增加了这个\u0026quot;随想\u0026quot;页面，希望可以更方便地记录日常的想法和感悟。\u003c/p\u003e\n\u003cp\u003e最近逐渐完善这个博客，希望后续自己能够多总结，多反思，拓展自己知识的广度和深度；\u003c/p\u003e","title":"博客随想功能上线了"},{"content":"","permalink":"http://localhost:13141/posts/%E9%A2%84%E7%A0%94%E6%8A%A5%E5%91%8A%E6%A8%A1%E6%9D%BF/","summary":"","title":"预研报告模板"},{"content":"Git工作流程：从入门到精通 Git是目前最流行的分布式版本控制系统，它不仅能够帮助开发者管理代码版本，还能促进团队协作。本文将详细介绍一下git的基础概念和在使用过程中的一些实用技巧。\n1. Git基础概念 1.1 什么是Git？ Git是一个开源的分布式版本控制系统，由Linus Torvalds于2005年创建。要理解Git，首先要明白什么是\u0026quot;版本控制\u0026quot;。\n版本控制：记录文件内容变化，以便将来查阅特定版本修订情况的系统。就像文档的\u0026quot;撤销\u0026quot;功能，但更强大。\n分布式：每个开发者都拥有完整的代码仓库副本，不像集中式系统（如SVN）那样只有一个中央服务器。\nGit的优势：\n速度快：大部分操作在本地完成 数据完整性：每个文件都有校验和，防止数据损坏 支持离线工作：不需要网络就能进行大部分操作 强大的分支功能：分支创建和合并非常高效 1.2 Git的基本工作区 Git有四个主要的工作区，理解这四个区域是掌握Git的关键：\n工作区(Working Directory) ↓ git add 暂存区(Staging Area/Index) ↓ git commit 本地仓库(Local Repository) ↓ git push 远程仓库(Remote Repository) 1. 工作区(Working Directory) 定义：你当前正在工作的目录，包含项目的所有文件 作用：在这里编辑代码、修改文件 特点：文件修改后，Git会检测到变化 2. 暂存区(Staging Area) 定义：也称为\u0026quot;索引(Index)\u0026quot;，是一个临时保存修改的地方 作用：在提交前，可以在这里选择要包含的修改（git add后存储的区域） 比喻：就像购物车，你可以把想买的东西先放进去，最后一起结账 3. 本地仓库(Local Repository) 定义：Git保存项目元数据和对象数据库的地方 作用：存储提交历史和文件快照（git commit） 位置：在项目目录的.git文件夹中 4. 远程仓库(Remote Repository) 定义：托管在GitHub、GitLab等平台上的仓库 作用：团队协作和备份（git push的地方） 特点：多个开发者可以共享同一个远程仓库 1.3 Git的基本工作流程 Git的基本工作流程很简单，只有四个核心步骤：\n在工作区修改文件 - 编辑代码、添加新文件等 使用git add将修改添加到暂存区 - 选择要提交的修改 使用git commit将暂存区的内容提交到本地仓库 - 创建一个版本快照 使用git push将本地仓库的修改推送到远程仓库 - 与团队共享 举个例子：\n1 2 3 4 5 6 7 8 9 10 11 # 1. 编辑文件（在工作区） echo \u0026#34;Hello Git\u0026#34; \u0026gt; README.md # 2. 添加到暂存区 git add README.md # 3. 提交到本地仓库 git commit -m \u0026#34;添加README文件\u0026#34; # 4. 推送到远程仓库 git push origin main 2. Git基本命令 2.1 初始化配置 配置用户信息 在使用Git之前，需要配置你的身份信息：\n1 2 3 4 5 6 7 8 9 10 11 12 # 配置全局用户名（所有仓库都使用这个名字） git config --global user.name \u0026#34;Your Name\u0026#34; # 配置全局邮箱（所有仓库都使用这个邮箱） git config --global user.email \u0026#34;your.email@example.com\u0026#34; # 查看当前配置 git config --list # 查看特定配置 git config user.name git config user.email 为什么需要配置？ Git需要知道是谁提交了代码，这样在查看历史时就能看到每个提交的作者信息。\n初始化仓库 1 2 3 4 5 # 在当前目录初始化Git仓库 git init # 克隆远程仓库（推荐新手使用） git clone https://github.com/username/repository.git git init vs git clone：\ngit init：在现有项目中创建Git仓库 git clone：复制现有的远程仓库到本地 2.2 基本操作 查看状态 1 2 3 4 5 6 7 8 9 10 # 查看工作区状态（最常用的命令） git status # 查看简化状态 git status -s # 简化输出，适合快速查看 # 输出示例： # M modified.txt # M表示已修改 # A new.txt # A表示已添加到暂存区 # ?? untracked.txt # ??表示未跟踪的文件 git status会告诉你什么？\n哪些文件被修改了 哪些文件在暂存区 哪些文件还未被Git跟踪 当前在哪个分支上 查看提交历史 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 查看详细提交历史 git log # 查看简洁提交历史（推荐） git log --oneline # 查看图形化提交历史（很有用） git log --graph --oneline --all # 查看最近n次提交 git log --oneline -5 # 查看特定文件的修改历史 git log filename 提交信息格式：\ncommit a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0 (HEAD -\u0026gt; main, origin/main) Author: Your Name \u0026lt;your.email@example.com\u0026gt; Date: Mon Oct 24 16:30:00 2025 +0800 提交信息：添加了新功能 添加和提交 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 添加指定文件到暂存区 git add filename # 添加所有修改到暂存区 git add . # 添加所有修改（包括删除的文件） git add -A # 提交暂存区内容 git commit -m \u0026#34;提交信息\u0026#34; # 跳过暂存区直接提交（提交所有已跟踪文件的修改） git commit -a -m \u0026#34;提交信息\u0026#34; # 修改最后一次提交信息（如果信息写错了） git commit --amend 提交信息的重要性：\n好的提交信息应该简洁明了 说明这次提交做了什么 遵循团队的提交规范 示例提交信息：\nfeat: 添加用户登录功能 - 实现登录表单验证 - 添加JWT token处理 - 完善错误提示信息 查看和比较 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 查看工作区与暂存区的差异（未暂存的修改） git diff # 查看暂存区与本地仓库的差异（已暂存但未提交的修改） git diff --staged # 查看工作区与本地仓库的差异（所有修改） git diff HEAD # 查看指定文件的差异 git diff filename # 查看两个提交之间的差异 git diff commit1 commit2 git diff输出解读：\n--- a/README.md +++ b/README.md @@ -1,3 +1,4 @@ # 项目说明 这是一个测试项目 +新增了一行内容 ---表示原始文件 +++表示修改后的文件 @@表示修改的位置 -表示删除的行 +表示新增的行 撤销操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 1. 撤销工作区的修改（恢复到最近一次提交状态） git checkout -- filename git checkout -- . # 撤销所有文件的修改 # 2. 撤销暂存区的修改（取消暂存，但保留工作区修改） git reset HEAD filename git reset HEAD . # 取消所有文件的暂存 # 3. 撤销最后一次提交（保留修改在工作区） git reset --soft HEAD~1 # 4. 撤销最后一次提交（丢弃修改） git reset --hard HEAD~1 # 5. 撤销多次提交（保留修改在工作区） git reset --soft HEAD~n # 6. 撤销多次提交（丢弃修改） git reset --hard HEAD~n 撤销操作的详细说明：\n1. git checkout -- \u0026lt;file\u0026gt; 作用：恢复工作区文件到HEAD状态，撤销文件的修改\n用法：\n1 2 3 4 5 # 撤销单个文件的修改 git checkout -- README.md # 撤销所有文件的修改 git checkout -- . 说明：只影响工作区，不能恢复未跟踪的新文件\n2. git reset HEAD \u0026lt;file\u0026gt; 作用：取消文件的暂存，但保留工作区的修改\n用法：\n1 2 3 4 5 # 取消暂存特定文件 git reset HEAD src/index.js # 取消所有文件的暂存 git reset HEAD . 说明：只影响暂存区，工作区和提交历史不变\n3. git reset --soft HEAD~1 作用：撤销最后一次提交，保留所有修改在工作区\n用法：\n1 2 3 4 5 # 撤销最后一次提交，保留修改 git reset --soft HEAD~1 # 重新编辑提交信息后提交 git commit -m \u0026#34;更正的提交信息\u0026#34; 说明：安全撤销提交，修改和暂存状态都保留\n4. git reset --hard HEAD~1 作用：彻底回到上一个提交，丢弃所有修改\n用法：\n1 2 3 4 5 # 彻底回到上一个提交（危险操作） git reset --hard HEAD~1 # 回到特定提交 git reset --hard abc1234 ⚠️ 警告：会丢失所有修改，执行前建议备份：\n1 git stash # 备份修改 5. git reset --soft/--hard HEAD~n 作用：撤销多个提交\n用法：\n1 2 3 4 5 # 撤销最近3次提交，保留修改 git reset --soft HEAD~3 # 撤销最近5次提交，丢弃修改 git reset --hard HEAD~5 说明：n是要回退的提交数量\n撤销操作选择指南 需求 命令 安全性 撤销文件修改 git checkout -- \u0026lt;file\u0026gt; 安全 取消文件暂存 git reset HEAD \u0026lt;file\u0026gt; 安全 撤销提交但保留修改 git reset --soft HEAD~1 安全 完全回到某状态 git reset --hard \u0026lt;commit\u0026gt; 危险 记忆技巧：\ncheckout → 工作区 reset HEAD → 暂存区 reset --soft → 提交历史（安全） reset --hard → 全部重置（危险） 2.3 远程仓库操作 添加和管理远程仓库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 查看远程仓库 git remote -v # 添加远程仓库 git remote add origin https://github.com/username/repository.git # 删除远程仓库 git remote remove origin # 修改远程仓库URL git remote set-url origin https://github.com/username/new-repository.git # 查看远程仓库详细信息 git remote show origin 远程仓库的命名：\norigin：默认的远程仓库名称 可以有多个远程仓库，如origin、upstream等 推送和拉取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 推送到远程仓库 git push origin main # 推送并设置上游分支（第一次推送时使用） git push -u origin main # 拉取远程仓库的修改（获取并合并） git pull origin main # 获取远程仓库的修改（不合并） git fetch origin # 合并远程分支到当前分支 git merge origin/main git pull vs git fetch：\ngit pull = git fetch + git merge git fetch：只获取远程更新，不合并 git pull：获取更新并自动合并 3. 分支管理 3.1 Git分支的基本定义 什么是分支？ 分支是Git最强大的功能之一。可以把分支想象成游戏的\u0026quot;存档点\u0026quot;，你可以在不同的分支上开发不同的功能，互不干扰。\n分支的比喻：\n树干：主分支（main/master） 树枝：功能分支、修复分支 树梢：最新的提交 本地分支 vs 远程分支 Git中有两种主要的分支类型：\n本地分支：存在于你本地仓库中的分支\nmain - 本地主分支 feature-branch - 本地功能分支 develop - 本地开发分支 远程分支：远程仓库（如GitHub）中的分支的本地引用\norigin/main - 远程仓库的main分支 origin/feature-branch - 远程仓库的功能分支 重要概念：\n本地分支是你可以直接操作的分支 远程分支是只读的，代表远程仓库的状态 origin/main不是真正的分支，而是对远程main分支的引用 远程仓库名的含义 origin：\n这是远程仓库的名称，不是分支 origin 是Git给默认远程仓库起的名字 它指向一个完整的远程仓库URL，比如：https://github.com/litianfugt/litianfugt.github.io.git blog：\n自定义的远程仓库名称 同样指向完整的远程仓库URL 可以根据项目需求自定义命名 git push origin main 详细解析 1 2 3 4 5 6 git push origin main │ │ │ │ │ │ │ └── 目标：远程仓库的main分支 │ │ └─────── 远程仓库名称 │ └─────────── Git命令：推送 └─────────────── Git工具 完整含义：将当前本地分支的内容推送到名为origin的远程仓库的main分支。\n等价写法：\n1 git push origin main:main # 完整语法：本地分支:远程分支 其他推送示例：\n1 2 3 4 5 6 7 8 # 推送本地分支到远程不同名称的分支 git push origin local-branch:remote-branch # 推送所有分支 git push --all origin # 推送并删除远程分支 git push origin --delete feature-branch remotes/blog/HEAD -\u0026gt; blog/main 的含义 符号引用解释：\nremotes/blog/HEAD -\u0026gt; blog/main │ │ │ │ │ │ │ └── 指向blog/main分支 │ │ └────── 符号引用指向 │ └─────────── 远程仓库blog的HEAD └─────────────────── 远程分支引用 实际意义：\nremotes/blog/HEAD：远程仓库blog的HEAD引用 -\u0026gt; blog/main：指向blog/main分支 作用：标识远程仓库的默认分支，简化某些Git操作 类似于桌面快捷方式，是一个符号引用 HEAD的概念：\nHEAD是一个指针，指向当前分支的最新提交 每个分支都有自己的HEAD 远程仓库的HEAD指向默认分支（通常是main） 3.2 分支的基本操作 创建和切换分支 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 创建新分支 git branch feature-branch # 切换到指定分支 git checkout feature-branch # 创建并切换到新分支（常用） git checkout -b feature-branch # 查看所有分支 git branch -a # 查看本地分支 git branch # 查看远程分支 git branch -r 分支命名规范：\nfeature/user-authentication # 功能分支 fix/login-bug # 修复分支 docs/api-documentation # 文档分支 refactor/user-service # 重构分支 hotfix/security-patch # 紧急修复分支 合并分支 1 2 3 4 5 6 7 8 9 10 11 # 切换到目标分支 git checkout main # 合并指定分支到当前分支 git merge feature-branch # 删除已合并的分支 git branch -d feature-branch # 强制删除分支（即使未合并） git branch -D feature-branch 合并的类型：\nFast-forward：快进合并，历史是线性的 Recursive：递归合并，创建合并提交 Squash：压缩合并，将多个提交合并为一个 解决合并冲突 当合并分支时，如果两个分支对同一文件的同一部分进行了不同的修改，就会产生合并冲突。\n冲突产生的原因：\n两个分支修改了同一文件的同一行 一个分支修改了文件，另一个分支删除了文件 文件重命名冲突 解决合并冲突的步骤：\n执行合并命令 1 git merge feature-branch 查看冲突状态 1 2 git status # 会显示 \u0026#34;both modified\u0026#34; 的文件 打开冲突文件，查看冲突标记 1 2 3 4 5 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 你的代码 ======= 别人的代码 \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; feature-branch 手动编辑文件，解决冲突 1 2 # 保留需要的代码，删除冲突标记 最终的结果代码 使用git add标记冲突已解决 1 git add conflicted-file 完成合并 1 git commit 冲突解决技巧：\n使用git diff查看具体冲突 与团队成员沟通确认保留哪些修改 使用IDE的合并工具辅助解决 3.3 变基(Rebase) 变基是将一系列提交应用到另一个分支上的操作，它可以使提交历史更加线性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 变基当前分支到目标分支 git rebase main # 变基指定分支到目标分支 git rebase main feature-branch # 交互式变基（可以编辑、删除、合并提交） git rebase -i HEAD~3 # 继续变基（解决冲突后） git rebase --continue # 取消变基 git rebase --abort 变基 vs 合并：\n合并：保留完整的历史，包括分支信息 变基：创建线性的历史，更整洁 建议：个人分支使用变基，公共分支使用合并 4. 常见问题解决 4.1 推送被拒绝（Push Rejected） 问题描述：\n1 2 3 4 5 6 git push origin main # 错误信息：! [rejected] main -\u0026gt; main (non-fast-forward) # error: failed to push some refs to \u0026#39;https://github.com/user/repo.git\u0026#39; # hint: Updates were rejected because the remote contains work that you do # hint: not have locally. You may want to first integrate the remote changes # hint: (e.g., \u0026#39;git pull ...\u0026#39;) before pushing again. 产生原因： 远程仓库有本地没有的提交，通常是其他团队成员推送了新的代码。\n解决方案：\n方法1：先拉取再推送（推荐）\n1 2 3 4 5 6 # 拉取远程更新 git pull origin main # 如果有冲突，解决冲突 # 然后推送 git push origin main 方法2：使用rebase保持历史整洁\n1 2 3 4 5 6 7 8 # 使用rebase拉取更新 git pull --rebase origin main # 如果有冲突，解决冲突 git rebase --continue # 推送 git push origin main 方法3：强制推送（谨慎使用）\n1 2 # 只有在你确定要覆盖远程历史时才使用 git push --force origin main 4.2 合并冲突（Merge Conflicts） 问题描述：\n1 2 3 4 git merge feature-branch # Auto-merging README.md # CONFLICT (content): Merge conflict in README.md # Automatic merge failed; fix conflicts and then commit the result. 详细解决步骤：\n查看冲突文件 1 2 git status # 输出：both modified: README.md 打开冲突文件，查看冲突标记 1 2 3 4 5 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 当前分支的内容 ======= 要合并分支的内容 \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; feature-branch 手动编辑文件解决冲突 1 2 # 删除冲突标记，保留需要的内容 最终合并后的内容 标记冲突已解决 1 git add README.md 完成合并 1 2 git commit # Git会自动生成合并提交信息 冲突解决技巧：\n使用git diff查看具体冲突内容 使用IDE的可视化合并工具 与团队成员沟通确认修改内容 4.3 远程仓库配置问题 修改远程仓库URL：\n1 2 3 4 5 6 7 8 # 查看当前远程仓库 git remote -v # 修改远程仓库URL git remote set-url origin https://github.com/user/new-repo.git # 验证修改 git remote -v 4.4 完全恢复工作区到初始状态 场景描述： 刚刚 git clone 下来的仓库，进行了修改后想要完全恢复到最初状态，但运行 git checkout -- . 后发现有些文件没有被恢复。\n问题原因： git checkout -- . 只能恢复已跟踪文件的修改，但无法删除新创建的文件和目录。这些新文件仍然存在于工作区中，可能导致项目运行异常。\n解决方案：\n1 2 3 4 5 # 步骤1：恢复已跟踪文件的修改 git checkout -- . # 步骤2：清理所有未跟踪的文件和目录 git clean -fd 完整流程示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 查看当前状态 git status # 输出：Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) # layouts/_default/ # 第一步：恢复已跟踪文件 git checkout -- . # 第二步：清理未跟踪文件 git clean -fd # 验证结果 git status # 输出：On branch main # Your branch is up to date with \u0026#39;origin/main\u0026#39;. # nothing to commit, working tree clean 命令参数说明：\ngit checkout -- .：恢复所有已跟踪文件的修改到最近一次提交状态 git clean -f：强制删除所有未跟踪的文件（-f = force） git clean -fd：删除所有未跟踪的文件和目录（-d = directory） 其他相关命令：\n1 2 3 4 5 6 7 8 9 10 11 # 查看将要删除的文件（不实际删除） git clean -n # 只删除忽略的文件 git clean -fx # 删除忽略和非忽略的文件，但不删除目录 git clean -fX # 手动删除特定目录 rm -rf layouts/_default/ 注意事项：\ngit clean 是一个破坏性操作，删除的文件无法通过Git恢复 建议先使用 git clean -n 预览将要删除的文件 确保新创建的文件确实不需要了再执行清理 记忆口诀：\n\u0026ldquo;先 checkout，再 clean，恢复完美如初见\u0026rdquo;\n4.5 提交信息写错了 修改最后一次提交信息：\n1 2 3 4 5 # 修改最后一次提交信息 git commit --amend # 如果已经推送，需要强制推送 git push --force origin main 修改更早的提交信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 交互式变基 git rebase -i HEAD~3 # 在编辑器中将要修改的提交前的pick改为edit # 保存退出后，Git会停在那个提交上 # 修改提交信息 git commit --amend # 继续变基 git rebase --continue # 如果需要强制推送 git push --force origin main 持续学习 目前只是了解部分git的基础概念和使用方式，后续在实践中还会继续总结一些使用方式，持续更新。\n","permalink":"http://localhost:13141/posts/git-workflow/","summary":"\u003ch1 id=\"git工作流程从入门到精通\"\u003eGit工作流程：从入门到精通\u003c/h1\u003e\n\u003cp\u003eGit是目前最流行的分布式版本控制系统，它不仅能够帮助开发者管理代码版本，还能促进团队协作。本文将详细介绍一下git的基础概念和在使用过程中的一些实用技巧。\u003c/p\u003e","title":"Git一些学习和使用记录"},{"content":"关于vision transformer的总结记录 本文主要总结一些优秀的视觉transformer，包括其实现原理，或是基于什么模型进行了哪些改进，一来便于后续做一些任务时的模型选型，同时也可以帮助后续优化模型提供一些方向，比如底层视觉任务中，需要模型更好的感知一些细节或者全局结构，这时候可以考虑在频域或者DCT余弦域做attention或者细节提取，然后再变换回去，抑或者是模型推理速度和模型效果的一些平衡问题。\n1. vision transformer的起点 ViT应该是vision transformer中最早能把相关任务效果做到SOTA的模型。在此之前还有一个开创性的工作iGPT值得关注。iGPT将图像像素按从左到右从上到下看成一个序列，这样就可以按照NLP的思路来学习相关特征了。iGPT尝试了GPT（AR，自回归）和Bert两种预训练方式，其探索了在没有使用2D结构先验信息的情况下，完全基于序列的方式也能学到很好的图像表征特征。但是其存在一个较大的问题\u0026ndash;输入图像尺寸的问题。由于其需要将输入图像降质为32x32等比较低的分辨率，其对高分辨率图像的特征学习能力有限。ViT也是想让模型在图像数据集上的预训练发挥更好的作用，同时解决对中高分辨率图像的表征作用。其核心思想是将图像分割成一个个pxp（16x16）大小的图像块（patch），然后将每个patch对应NLP领域中的词（word）。先对每个patch进行向量嵌入（patch embedding），再叠加一个相同维度可学习的位置向量（position embedding），然后按照Bert的预训练方式在大型图像数据集上训练ViT模型，当然其进行的预训练包括分类，其在输入向量最前面嵌入了一个【class】向量，作为预训练或者微调时分类头的输入。ViT最大的贡献有三点，其一，提出了一种更有效的在图像领域预训练transformer的方法，并且基于预训练后的模型在特定任务上微调可以比肩甚至超过当前最好的CNN模型。其二，证实了类似于卷积这种能先天利用二维信息的偏置只是能在训练数据量较小时提供一个更好的先验信息而已，在较大数据集上预训练ViT能提供同等甚至更好的先验信息，其可学习的位置向量最后展现出了明显的2D信息。其三，虽然预训练对CNN模型也有效果提升，但是在相同计算资源的情况下，预训练ViT的收益更大，此外，当不受数据量和训练资源限制时，ViT的性能提升也没有出现像CNN模型那样的饱和情况，其在更大规模的数据集上的应用前景更好。\nViT告诉我们只要在足够大的数据集上预训练，纯transformer架构的网络能够在视觉任务上比肩甚至超越现阶段最好的卷积网络。然而，在大规模数据集上预训练是一件非常费事费资源的事情。这也非常限制transformer在视觉任务中的使用。为了解决这个问题，DeiT尝试通过一些优化策略和引入创新的蒸馏方法，使得transformer模型在不预训练的情况下仅使用相同规模数据集也能达到SOTA性能。其核心点是使用一个卷积网络（如RegNeY-16GF）作为教师模型去辅助训练ViT，将一些卷积具有的归纳偏置传递给ViT。实现这一点的创新蒸馏方式则是引入一个额外的蒸馏token，类似于class token，让其对齐教师模型预测的硬标签（论文指出硬标签比软标签的效果好），同时结合class token与实际类别的联合约束，最终使得纯transformer模型能够在小规模数据集上达到SOTA性能。\n","permalink":"http://localhost:13141/posts/vision_transformer_summarys/","summary":"\u003ch1 id=\"关于vision-transformer的总结记录\"\u003e关于vision transformer的总结记录\u003c/h1\u003e\n\u003cp\u003e本文主要总结一些优秀的视觉transformer，包括其实现原理，或是基于什么模型进行了哪些改进，一来便于后续做一些任务时的模型选型，同时也可以帮助后续优化模型提供一些方向，比如底层视觉任务中，需要模型更好的感知一些细节或者全局结构，这时候可以考虑在频域或者DCT余弦域做attention或者细节提取，然后再变换回去，抑或者是模型推理速度和模型效果的一些平衡问题。\u003c/p\u003e","title":"vision transformer summary"},{"content":"计算机视觉基础：从像素到理解 计算机视觉是人工智能领域的一个重要分支，它致力于让计算机能够从图像或视频中获取信息、理解内容并做出决策。从简单的图像处理到复杂的场景理解，计算机视觉技术已经广泛应用于医疗诊断、自动驾驶、安防监控、人脸识别等众多领域。本文将全面介绍计算机视觉的基础知识，帮助读者理解计算机如何\u0026quot;看懂\u0026quot;图像。\n图像基础 图像表示 数字图像的概念 数字图像是由有限数量的像素（Picture Element，简称Pixel）组成的二维矩阵。每个像素代表图像中的一个点，具有特定的位置和值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import matplotlib.pyplot as plt # 创建一个简单的灰度图像 # 5x5的灰度图像，值范围0-255 gray_image = np.array([ [0, 50, 100, 150, 200], [30, 80, 130, 180, 230], [60, 110, 160, 210, 240], [90, 140, 190, 220, 250], [120, 170, 200, 230, 255] ], dtype=np.uint8) # 判空处理 assert gray_image is not None, \u0026#34;灰度图像创建失败！\u0026#34; # 显示图像 plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Grayscale Image\u0026#39;) plt.colorbar() plt.show() 彩色图像表示 彩色图像通常使用RGB（红、绿、蓝）三个通道来表示。每个像素由三个值组成，分别代表红、绿、蓝三个颜色通道的强度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 创建一个简单的彩色图像 # 5x5x3的RGB图像，值范围0-255 color_image = np.zeros((5, 5, 3), dtype=np.uint8) # 判空处理 assert color_image is not None, \u0026#34;彩色图像创建失败！\u0026#34; # 设置红色通道 color_image[:, :, 0] = np.array([ [255, 200, 150, 100, 50], [230, 180, 130, 80, 30], [210, 160, 110, 60, 10], [190, 140, 90, 40, 0], [170, 120, 70, 20, 0] ]) # 设置绿色通道 color_image[:, :, 1] = np.array([ [0, 50, 100, 150, 200], [30, 80, 130, 180, 230], [60, 110, 160, 210, 240], [90, 140, 190, 220, 250], [120, 170, 200, 230, 255] ]) # 设置蓝色通道 color_image[:, :, 2] = np.array([ [0, 30, 60, 90, 120], [50, 80, 110, 140, 170], [100, 130, 160, 190, 200], [150, 180, 210, 220, 230], [200, 230, 240, 250, 255] ]) # 显示图像 plt.imshow(color_image) plt.title(\u0026#39;Color Image\u0026#39;) plt.show() 其他颜色空间 除了RGB，还有其他常用的颜色空间，如HSV（色相、饱和度、明度）和Lab（亮度、a通道、b通道）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import cv2 # 将RGB图像转换为HSV颜色空间 hsv_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2HSV) # 将RGB图像转换为Lab颜色空间 lab_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2Lab) # 显示不同颜色空间的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(color_image) plt.title(\u0026#39;RGB Image\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(hsv_image) plt.title(\u0026#39;HSV Image\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(lab_image) plt.title(\u0026#39;Lab Image\u0026#39;) plt.tight_layout() plt.show() 图像属性 分辨率 图像分辨率是指图像中像素的数量，通常表示为宽度×高度（如1920×1080）。高分辨率图像包含更多细节，但也需要更多的存储空间和处理时间。\n1 2 3 4 5 6 # 获取图像分辨率 height, width = gray_image.shape print(f\u0026#34;灰度图像分辨率: {width}x{height}\u0026#34;) height, width, channels = color_image.shape print(f\u0026#34;彩色图像分辨率: {width}x{height}, 通道数: {channels}\u0026#34;) 位深度 位深度是指每个像素使用的位数，决定了图像可以表示的颜色数量。常见的位深度有8位（256个灰度级）、24位（RGB各8位，约1670万种颜色）等。\n1 2 3 4 5 6 7 8 9 10 # 检查图像的位深度 print(f\u0026#34;灰度图像数据类型: {gray_image.dtype}\u0026#34;) print(f\u0026#34;彩色图像数据类型: {color_image.dtype}\u0026#34;) # 计算可以表示的颜色数量 gray_levels = 2 ** (gray_image.itemsize * 8) color_levels = 2 ** (color_image.itemsize * 8) print(f\u0026#34;灰度图像可以表示的灰度级数: {gray_levels}\u0026#34;) print(f\u0026#34;彩色图像每个通道可以表示的颜色级数: {color_levels}\u0026#34;) 图像基本处理 图像读取与显示 使用OpenCV读取图像 OpenCV是一个广泛使用的计算机视觉库，提供了丰富的图像处理功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import cv2 # 读取图像 # 注意：OpenCV默认以BGR格式读取彩色图像 image_bgr = cv2.imread(\u0026#39;example.jpg\u0026#39;) # 检查图像是否成功读取 if image_bgr is None: print(\u0026#34;无法读取图像\u0026#34;) else: # 转换为RGB格式以便正确显示 image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) # 显示图像 plt.imshow(image_rgb) plt.title(\u0026#39;Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) # 不显示坐标轴 plt.show() 使用PIL/Pillow读取图像 Pillow是Python图像处理库，提供了简单易用的图像处理功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from PIL import Image # 读取图像 image = Image.open(\u0026#39;example.jpg\u0026#39;) # 显示图像 image.show() # 转换为numpy数组 image_array = np.array(image) # 显示图像信息 print(f\u0026#34;图像大小: {image.size}\u0026#34;) print(f\u0026#34;图像模式: {image.mode}\u0026#34;) print(f\u0026#34;图像数组形状: {image_array.shape}\u0026#34;) 图像基本操作 裁剪图像 1 2 3 4 5 6 7 8 # 裁剪图像 (y1:y2, x1:x2) cropped_image = image_rgb[50:200, 100:300] # 显示裁剪后的图像 plt.imshow(cropped_image) plt.title(\u0026#39;Cropped Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() 调整图像大小 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 使用OpenCV调整图像大小 resized_cv2 = cv2.resize(image_rgb, (300, 200)) # 使用PIL调整图像大小 resized_pil = Image.fromarray(image_rgb).resize((300, 200)) # 显示调整大小后的图像 plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.imshow(resized_cv2) plt.title(\u0026#39;Resized with OpenCV\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(resized_pil) plt.title(\u0026#39;Resized with PIL\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 旋转图像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 使用OpenCV旋转图像 (h, w) = image_rgb.shape[:2] center = (w // 2, h // 2) M = cv2.getRotationMatrix2D(center, 45, 1.0) rotated_cv2 = cv2.warpAffine(image_rgb, M, (w, h)) # 使用PIL旋转图像 rotated_pil = Image.fromarray(image_rgb).rotate(45) # 显示旋转后的图像 plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.imshow(rotated_cv2) plt.title(\u0026#39;Rotated with OpenCV\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(rotated_pil) plt.title(\u0026#39;Rotated with PIL\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 翻转图像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 水平翻转 flipped_h = cv2.flip(image_rgb, 1) # 垂直翻转 flipped_v = cv2.flip(image_rgb, 0) # 水平和垂直翻转 flipped_hv = cv2.flip(image_rgb, -1) # 显示翻转后的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(flipped_h) plt.title(\u0026#39;Horizontal Flip\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(flipped_v) plt.title(\u0026#39;Vertical Flip\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(flipped_hv) plt.title(\u0026#39;Horizontal and Vertical Flip\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 图像增强 亮度调整 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 增加亮度 brightness_increase = cv2.convertScaleAbs(image_rgb, alpha=1.2, beta=50) # 减少亮度 brightness_decrease = cv2.convertScaleAbs(image_rgb, alpha=1.0, beta=-50) # 显示亮度调整后的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(brightness_increase) plt.title(\u0026#39;Increased Brightness\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(brightness_decrease) plt.title(\u0026#39;Decreased Brightness\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 对比度调整 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 增加对比度 contrast_increase = cv2.convertScaleAbs(image_rgb, alpha=1.5, beta=0) # 减少对比度 contrast_decrease = cv2.convertScaleAbs(image_rgb, alpha=0.5, beta=0) # 显示对比度调整后的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(contrast_increase) plt.title(\u0026#39;Increased Contrast\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(contrast_decrease) plt.title(\u0026#39;Decreased Contrast\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 直方图均衡化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 转换为灰度图像 gray_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY) # 直方图均衡化 equalized_image = cv2.equalizeHist(gray_image) # 显示直方图均衡化前后的图像和直方图 plt.figure(figsize=(15, 10)) # 原始图像 plt.subplot(2, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Grayscale Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) # 均衡化后的图像 plt.subplot(2, 2, 2) plt.imshow(equalized_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Equalized Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) # 原始直方图 plt.subplot(2, 2, 3) plt.hist(gray_image.ravel(), 256, [0, 256]) plt.title(\u0026#39;Original Histogram\u0026#39;) # 均衡化后的直方图 plt.subplot(2, 2, 4) plt.hist(equalized_image.ravel(), 256, [0, 256]) plt.title(\u0026#39;Equalized Histogram\u0026#39;) plt.tight_layout() plt.show() 伽马校正 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def adjust_gamma(image, gamma=1.0): # 构建查找表 inv_gamma = 1.0 / gamma table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\u0026#34;uint8\u0026#34;) # 应用伽马校正 return cv2.LUT(image, table) # 应用不同的伽马值 gamma_1_5 = adjust_gamma(image_rgb, 1.5) gamma_0_5 = adjust_gamma(image_rgb, 0.5) # 显示伽马校正后的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image (γ=1.0)\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(gamma_1_5) plt.title(\u0026#39;Gamma=1.5\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(gamma_0_5) plt.title(\u0026#39;Gamma=0.5\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 图像滤波 均值滤波 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 转换为灰度图像 gray_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY) # 应用不同大小的均值滤波 blur_3x3 = cv2.blur(gray_image, (3, 3)) blur_5x5 = cv2.blur(gray_image, (5, 5)) blur_7x7 = cv2.blur(gray_image, (7, 7)) # 显示均值滤波后的图像 plt.figure(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 2) plt.imshow(blur_3x3, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;3x3 Mean Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 3) plt.imshow(blur_5x5, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;5x5 Mean Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 4) plt.imshow(blur_7x7, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;7x7 Mean Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 高斯滤波 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 应用不同大小和标准差的高斯滤波 gaussian_3x3 = cv2.GaussianBlur(gray_image, (3, 3), 0) gaussian_5x5 = cv2.GaussianBlur(gray_image, (5, 5), 0) gaussian_7x7 = cv2.GaussianBlur(gray_image, (7, 7), 0) # 显示高斯滤波后的图像 plt.figure(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 2) plt.imshow(gaussian_3x3, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;3x3 Gaussian Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 3) plt.imshow(gaussian_5x5, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;5x5 Gaussian Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 4) plt.imshow(gaussian_7x7, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;7x7 Gaussian Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 中值滤波 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 应用不同大小的中值滤波 median_3 = cv2.medianBlur(gray_image, 3) median_5 = cv2.medianBlur(gray_image, 5) median_7 = cv2.medianBlur(gray_image, 7) # 显示中值滤波后的图像 plt.figure(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 2) plt.imshow(median_3, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;3x3 Median Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 3) plt.imshow(median_5, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;5x5 Median Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 4) plt.imshow(median_7, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;7x7 Median Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 双边滤波 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 应用双边滤波 bilateral = cv2.bilateralFilter(gray_image, 9, 75, 75) # 显示双边滤波后的图像 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(bilateral, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Bilateral Filter\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 边缘检测 Sobel算子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # 应用Sobel算子 sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3) sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3) sobel_xy = cv2.Sobel(gray_image, cv2.CV_64F, 1, 1, ksize=3) # 转换回uint8 sobel_x = cv2.convertScaleAbs(sobel_x) sobel_y = cv2.convertScaleAbs(sobel_y) sobel_xy = cv2.convertScaleAbs(sobel_xy) # 显示Sobel边缘检测结果 plt.figure(figsize=(15, 10)) plt.subplot(2, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 2) plt.imshow(sobel_x, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Sobel X\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 3) plt.imshow(sobel_y, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Sobel Y\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 2, 4) plt.imshow(sobel_xy, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Sobel XY\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() Laplacian算子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 应用Laplacian算子 laplacian = cv2.Laplacian(gray_image, cv2.CV_64F) laplacian = cv2.convertScaleAbs(laplacian) # 显示Laplacian边缘检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(laplacian, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Laplacian Edge Detection\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() Canny边缘检测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 应用Canny边缘检测 canny_low = cv2.Canny(gray_image, 50, 150) canny_high = cv2.Canny(gray_image, 100, 200) # 显示Canny边缘检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(canny_low, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Canny (50, 150)\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(canny_high, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Canny (100, 200)\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 图像分割 阈值分割 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 应用不同类型的阈值分割 ret, thresh_binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY) ret, thresh_binary_inv = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY_INV) ret, thresh_trunc = cv2.threshold(gray_image, 127, 255, cv2.THRESH_TRUNC) ret, thresh_tozero = cv2.threshold(gray_image, 127, 255, cv2.THRESH_TOZERO) ret, thresh_tozero_inv = cv2.threshold(gray_image, 127, 255, cv2.THRESH_TOZERO_INV) # 显示阈值分割结果 plt.figure(figsize=(15, 10)) plt.subplot(2, 3, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 3, 2) plt.imshow(thresh_binary, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Binary Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 3, 3) plt.imshow(thresh_binary_inv, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Binary Inverted Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 3, 4) plt.imshow(thresh_trunc, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Truncated Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 3, 5) plt.imshow(thresh_tozero, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;To Zero Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(2, 3, 6) plt.imshow(thresh_tozero_inv, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;To Zero Inverted Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 自适应阈值分割 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 应用自适应阈值分割 adaptive_mean = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2) adaptive_gaussian = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2) # 显示自适应阈值分割结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(adaptive_mean, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Adaptive Mean Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(adaptive_gaussian, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Adaptive Gaussian Threshold\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() Otsu阈值分割 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 应用Otsu阈值分割 ret, otsu = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # 显示Otsu阈值分割结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(otsu, cmap=\u0026#39;gray\u0026#39;) plt.title(f\u0026#39;Otsu Threshold (Threshold={ret})\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 分水岭算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # 创建一个简单的二值图像 binary_image = np.zeros((300, 300), dtype=np.uint8) cv2.circle(binary_image, (100, 100), 50, 255, -1) cv2.circle(binary_image, (200, 200), 50, 255, -1) # 应用距离变换 dist_transform = cv2.distanceTransform(binary_image, cv2.DIST_L2, 5) ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0) sure_fg = np.uint8(sure_fg) # 未知区域 unknown = cv2.subtract(binary_image, sure_fg) # 标记标签 ret, markers = cv2.connectedComponents(sure_fg) markers = markers + 1 markers[unknown == 255] = 0 # 应用分水岭算法 markers = cv2.watershed(cv2.cvtColor(binary_image, cv2.COLOR_GRAY2BGR), markers) # 显示分水岭算法结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plt.imshow(binary_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Binary Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 2) plt.imshow(dist_transform, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Distance Transform\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 3, 3) plt.imshow(markers, cmap=\u0026#39;jet\u0026#39;) plt.title(\u0026#39;Watershed Segmentation\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 特征提取 Harris角点检测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 应用Harris角点检测 gray_float = np.float32(gray_image) harris_corners = cv2.cornerHarris(gray_float, 2, 3, 0.04) # 扩大角点标记 harris_corners = cv2.dilate(harris_corners, None) # 设置阈值 threshold = 0.01 * harris_corners.max() corner_image = image_rgb.copy() corner_image[harris_corners \u0026gt; threshold] = [255, 0, 0] # 显示Harris角点检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(corner_image) plt.title(\u0026#39;Harris Corner Detection\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() Shi-Tomasi角点检测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 应用Shi-Tomasi角点检测 corners = cv2.goodFeaturesToTrack(gray_image, 100, 0.01, 10) corners = np.int0(corners) # 绘制角点 shi_tomasi_image = image_rgb.copy() for corner in corners: x, y = corner.ravel() cv2.circle(shi_tomasi_image, (x, y), 3, 255, -1) # 显示Shi-Tomasi角点检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(shi_tomasi_image) plt.title(\u0026#39;Shi-Tomasi Corner Detection\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() SIFT特征提取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 创建SIFT对象 sift = cv2.SIFT_create() # 检测关键点和计算描述符 keypoints, descriptors = sift.detectAndCompute(gray_image, None) # 绘制关键点 sift_image = cv2.drawKeypoints(gray_image, keypoints, None) # 显示SIFT特征提取结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(sift_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;SIFT Feature Extraction\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() ORB特征提取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 创建ORB对象 orb = cv2.ORB_create() # 检测关键点和计算描述符 keypoints, descriptors = orb.detectAndCompute(gray_image, None) # 绘制关键点 orb_image = cv2.drawKeypoints(gray_image, keypoints, None) # 显示ORB特征提取结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(gray_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(orb_image, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;ORB Feature Extraction\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() 目标检测 Haar级联分类器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 加载Haar级联分类器 face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_frontalface_default.xml\u0026#39;) eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_eye.xml\u0026#39;) # 检测人脸和眼睛 faces = face_cascade.detectMultiScale(gray_image, 1.3, 5) face_eye_image = image_rgb.copy() for (x, y, w, h) in faces: cv2.rectangle(face_eye_image, (x, y), (x+w, y+h), (255, 0, 0), 2) roi_gray = gray_image[y:y+h, x:x+w] roi_color = face_eye_image[y:y+h, x:x+w] eyes = eye_cascade.detectMultiScale(roi_gray) for (ex, ey, ew, eh) in eyes: cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2) # 显示Haar级联分类器检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(face_eye_image) plt.title(\u0026#39;Haar Cascade Detection\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() HOG特征与SVM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from skimage.feature import hog from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 提取HOG特征 def extract_hog_features(images): features = [] for image in images: # 计算HOG特征 fd = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False) features.append(fd) return np.array(features) # 假设我们有一些标记的图像数据 # 这里只是示例，实际应用中需要真实数据 # X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2) # 提取训练和测试集的HOG特征 # X_train_hog = extract_hog_features(X_train) # X_test_hog = extract_hog_features(X_test) # 训练SVM分类器 # svm = SVC(kernel=\u0026#39;linear\u0026#39;) # svm.fit(X_train_hog, y_train) # 在测试集上评估 # y_pred = svm.predict(X_test_hog) # accuracy = accuracy_score(y_test, y_pred) # print(f\u0026#34;Accuracy: {accuracy}\u0026#34;) 深度学习目标检测 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # 这里只是示例代码，实际应用中需要安装相应的深度学习框架 # 如TensorFlow或PyTorch，以及预训练模型 # 使用TensorFlow和预训练的SSD模型 \u0026#34;\u0026#34;\u0026#34; import tensorflow as tf # 加载预训练的SSD模型 model = tf.saved_model.load(\u0026#39;ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model\u0026#39;) # 预处理图像 input_tensor = tf.convert_to_tensor(image_rgb) input_tensor = input_tensor[tf.newaxis, ...] # 运行模型 detections = model(input_tensor) # 解析检测结果 num_detections = int(detections.pop(\u0026#39;num_detections\u0026#39;)) detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()} detections[\u0026#39;num_detections\u0026#39;] = num_detections # 过滤检测结果 min_score_thresh = 0.5 detections[\u0026#39;detection_classes\u0026#39;] = detections[\u0026#39;detection_classes\u0026#39;].astype(np.int64) indexes = np.where(detections[\u0026#39;detection_scores\u0026#39;] \u0026gt; min_score_thresh)[0] # 绘制检测结果 result_image = image_rgb.copy() for i in indexes: class_id = detections[\u0026#39;detection_classes\u0026#39;][i] score = detections[\u0026#39;detection_scores\u0026#39;][i] bbox = detections[\u0026#39;detection_boxes\u0026#39;][i] # 将归一化的边界框转换为像素坐标 h, w, _ = image_rgb.shape y1, x1, y2, x2 = bbox y1, x1, y2, x2 = int(y1 * h), int(x1 * w), int(y2 * h), int(x2 * w) # 绘制边界框和标签 cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2) label = f\u0026#34;{class_id}: {score:.2f}\u0026#34; cv2.putText(result_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) # 显示检测结果 plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) plt.imshow(image_rgb) plt.title(\u0026#39;Original Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(1, 2, 2) plt.imshow(result_image) plt.title(\u0026#39;Deep Learning Object Detection\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() \u0026#34;\u0026#34;\u0026#34; 总结 计算机视觉是一个广泛而深入的领域，本文介绍了从基础的图像表示和处理到高级的特征提取和目标检测的基本概念和方法。通过学习这些基础知识，读者可以为进一步探索计算机视觉的更高级主题打下坚实的基础。\n随着深度学习技术的发展，计算机视觉领域正在经历快速变革。传统的计算机视觉方法与深度学习相结合，正在推动计算机视觉在各个领域的应用不断拓展。希望本文能够帮助读者理解计算机视觉的基本原理，并激发进一步学习和探索的兴趣。\n在未来，计算机视觉技术将继续发展，在自动驾驶、医疗诊断、增强现实、机器人技术等领域发挥越来越重要的作用。掌握计算机视觉的基础知识，将为读者在这一充满机遇的领域中发展提供有力支持。\n","permalink":"http://localhost:13141/posts/computer-vision-basics/","summary":"\u003ch1 id=\"计算机视觉基础从像素到理解\"\u003e计算机视觉基础：从像素到理解\u003c/h1\u003e\n\u003cp\u003e计算机视觉是人工智能领域的一个重要分支，它致力于让计算机能够从图像或视频中获取信息、理解内容并做出决策。从简单的图像处理到复杂的场景理解，计算机视觉技术已经广泛应用于医疗诊断、自动驾驶、安防监控、人脸识别等众多领域。本文将全面介绍计算机视觉的基础知识，帮助读者理解计算机如何\u0026quot;看懂\u0026quot;图像。\u003c/p\u003e","title":"计算机视觉基础：从像素到理解"},{"content":"深度学习在图像处理中的应用：从CNN到GAN 深度学习技术的快速发展为图像处理领域带来了革命性的变化。从图像分类到目标检测，从图像分割到图像生成，深度学习模型在各项任务中都取得了突破性的进展。本文将探讨深度学习在图像处理中的主要应用，重点关注卷积神经网络(CNN)和生成对抗网络(GAN)等核心模型。\n深度学习与图像处理 传统图像处理的局限性 传统图像处理方法主要依赖于手工设计的特征提取器和算法，这些方法虽然在特定任务上表现良好，但存在以下局限性：\n特征设计困难：需要领域专家设计特征，耗时且难以泛化。 适应性差：对光照、角度、尺度等变化敏感。 复杂场景处理能力有限：难以处理复杂背景和多变的环境。 端到端学习困难：通常需要多个步骤组合，难以实现端到端优化。 深度学习的优势 深度学习，特别是深度神经网络，通过自动学习特征表示，克服了传统方法的许多局限：\n自动特征提取：无需人工设计特征，网络自动学习最优表示。 强大的表示能力：多层网络结构可以学习复杂的特征层次。 端到端学习：从原始输入到最终输出，整个过程可优化。 适应性强：对各种变化具有更好的鲁棒性。 大数据驱动：能够利用大量数据进行学习，提高泛化能力。 卷积神经网络(CNN) 卷积神经网络是深度学习在图像处理领域最成功的应用之一，其设计灵感来源于生物视觉系统。\nCNN的基本结构 典型的CNN由以下几种层组成：\n卷积层(Convolutional Layer)：使用卷积核提取局部特征。 池化层(Pooling Layer)：降低空间维度，减少计算量。 激活函数层(Activation Layer)：引入非线性，增强模型表达能力。 全连接层(Fully Connected Layer)：整合特征，进行最终分类或回归。 归一化层(Normalization Layer)：如批归一化(Batch Normalization)，加速训练。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # 使用PyTorch构建简单的CNN import torch import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self, num_classes=10): super(SimpleCNN, self).__init__() self.features = nn.Sequential( # 卷积层1 nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # 卷积层2 nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # 卷积层3 nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2) ) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(128 * 28 * 28, 512), # 输入尺寸需与特征图尺寸一致 nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(512, num_classes) ) def forward(self, x): # x: 输入张量，形状为 (batch_size, 3, 224, 224) 或根据实际输入调整 # 返回分类结果 x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x 经典CNN架构 LeNet-5 LeNet-5是最早的卷积神经网络之一，由Yann LeCun在1998年提出，主要用于手写数字识别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class LeNet5(nn.Module): def __init__(self): super(LeNet5, self).__init__() self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2) self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1) self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = self.pool1(x) x = torch.relu(self.conv2(x)) x = self.pool2(x) x = x.view(-1, 16 * 5 * 5) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x AlexNet AlexNet在2012年ImageNet竞赛中取得了突破性成绩，标志着深度学习在计算机视觉领域的崛起。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AlexNet(nn.Module): def __init__(self, num_classes=1000): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x VGGNet VGGNet以其简洁的结构和出色的性能著称，主要特点是使用小尺寸卷积核和深层网络。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class VGG16(nn.Module): def __init__(self, num_classes=1000): super(VGG16, self).__init__() self.features = nn.Sequential( # Block 1 nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # Block 2 nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # Block 3 nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # Block 4 nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), # Block 5 nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), ) self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.classifier(x) return x ResNet ResNet通过引入残差连接解决了深层网络训练中的梯度消失问题，使得构建数百甚至上千层的网络成为可能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class BasicBlock(nn.Module): expansion = 1 def __init__(self, in_channels, out_channels, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channels) self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = nn.ReLU(inplace=True)(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = nn.ReLU(inplace=True)(out) return out class ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000): super(ResNet, self).__init__() self.in_channels = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) def _make_layer(self, block, channels, blocks, stride=1): downsample = None if stride != 1 or self.in_channels != channels * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channels * block.expansion), ) layers = [] layers.append(block(self.in_channels, channels, stride, downsample)) self.in_channels = channels * block.expansion for _ in range(1, blocks): layers.append(block(self.in_channels, channels)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = nn.ReLU(inplace=True)(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x def resnet18(): return ResNet(BasicBlock, [2, 2, 2, 2]) CNN在图像处理中的应用 图像分类 图像分类是CNN最基本的应用，通过训练网络识别图像中的主要对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 使用预训练的ResNet进行图像分类 import torchvision.models as models import torchvision.transforms as transforms from PIL import Image # 加载预训练模型 model = models.resnet18(pretrained=True) model.eval() # 图像预处理 preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) # 加载图像 image = Image.open(\u0026#34;example.jpg\u0026#34;) input_tensor = preprocess(image) input_batch = input_tensor.unsqueeze(0) # 预测 with torch.no_grad(): output = model(input_batch) # 获取预测结果 _, predicted_idx = torch.max(output, 1) 目标检测 目标检测不仅识别图像中的对象，还确定它们的位置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 使用Faster R-CNN进行目标检测 import torchvision from torchvision.models.detection import fasterrcnn_resnet50_fpn # 加载预训练模型 model = fasterrcnn_resnet50_fpn(pretrained=True) model.eval() # 图像预处理 transform = transforms.Compose([transforms.ToTensor()]) # 加载图像 image = Image.open(\u0026#34;example.jpg\u0026#34;) image_tensor = transform(image).unsqueeze(0) # 预测 with torch.no_grad(): predictions = model(image_tensor) 图像分割 图像分割将图像划分为多个区域或对象，包括语义分割和实例分割。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 使用FCN进行语义分割 from torchvision.models.segmentation import fcn # 加载预训练模型 model = fcn.fcn_resnet50(pretrained=True) model.eval() # 图像预处理 preprocess = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) # 加载图像 image = Image.open(\u0026#34;example.jpg\u0026#34;) input_tensor = preprocess(image).unsqueeze(0) # 预测 with torch.no_grad(): output = model(input_tensor)[\u0026#39;out\u0026#39;] 生成对抗网络(GAN) 生成对抗网络是由Ian Goodfellow在2014年提出的一种深度学习模型，通过生成器和判别器的对抗训练，能够生成逼真的图像。\nGAN的基本原理 GAN由两个神经网络组成：\n生成器(Generator)：试图生成逼真的数据，以欺骗判别器。 判别器(Discriminator)：试图区分真实数据和生成器生成的假数据。 这两个网络通过对抗训练不断改进，最终生成器能够生成与真实数据分布相似的样本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 简单的GAN实现 import torch import torch.nn as nn class Generator(nn.Module): def __init__(self, latent_dim, img_shape): super(Generator, self).__init__() self.img_shape = img_shape def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *self.img_shape) return img class Discriminator(nn.Module): def __init__(self, img_shape): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity GAN的训练过程 GAN的训练过程是一个极小极大博弈问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # GAN训练循环 import torch.optim as optim # 初始化模型和优化器 latent_dim = 100 img_shape = (1, 28, 28) # MNIST图像大小 generator = Generator(latent_dim, img_shape) discriminator = Discriminator(img_shape) # 优化器 optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999)) optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)) # 损失函数 adversarial_loss = torch.nn.BCELoss() # 训练参数 n_epochs = 200 batch_size = 64 for epoch in range(n_epochs): for i, (imgs, _) in enumerate(dataloader): # 真实和假的标签 real = torch.ones(imgs.size(0), 1) fake = torch.zeros(imgs.size(0), 1) # 训练生成器 optimizer_G.zero_grad() z = torch.randn(imgs.size(0), latent_dim) gen_imgs = generator(z) g_loss = adversarial_loss(discriminator(gen_imgs), real) g_loss.backward() optimizer_G.step() # 训练判别器 optimizer_D.zero_grad() real_loss = adversarial_loss(discriminator(imgs), real) fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() 常见的GAN变体 DCGAN (Deep Convolutional GAN) DCGAN将CNN结构引入GAN，提高了生成图像的质量和训练稳定性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class DCGAN_Generator(nn.Module): def __init__(self, latent_dim, channels=1): super(DCGAN_Generator, self).__init__() self.init_size = 7 # 初始大小 self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2)) self.conv_blocks = nn.Sequential( nn.BatchNorm2d(128), nn.Upsample(scale_factor=2), nn.Conv2d(128, 128, 3, stride=1, padding=1), nn.BatchNorm2d(128, 0.8), nn.LeakyReLU(0.2, inplace=True), nn.Upsample(scale_factor=2), nn.Conv2d(128, 64, 3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(64, channels, 3, stride=1, padding=1), nn.Tanh(), ) def forward(self, z): out = self.l1(z) out = out.view(out.shape[0], 128, self.init_size, self.init_size) img = self.conv_blocks(out) return img CycleGAN CycleGAN用于在没有成对训练数据的情况下进行图像到图像的转换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class ResidualBlock(nn.Module): def __init__(self, in_features): super(ResidualBlock, self).__init__() self.block = nn.Sequential( nn.ReflectionPad2d(1), nn.Conv2d(in_features, in_features, 3), nn.InstanceNorm2d(in_features), nn.ReLU(inplace=True), nn.ReflectionPad2d(1), nn.Conv2d(in_features, in_features, 3), nn.InstanceNorm2d(in_features) ) def forward(self, x): return x + self.block(x) class GeneratorResNet(nn.Module): def __init__(self, input_shape, num_residual_blocks): super(GeneratorResNet, self).__init__() channels = input_shape[0] # 初始卷积块 out_features = 64 model = [ nn.ReflectionPad2d(3), nn.Conv2d(channels, out_features, 7), nn.InstanceNorm2d(out_features), nn.ReLU(inplace=True) ] in_features = out_features # 下采样 for _ in range(2): out_features *= 2 model += [ nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.InstanceNorm2d(out_features), nn.ReLU(inplace=True) ] in_features = out_features # 残差块 for _ in range(num_residual_blocks): model += [ResidualBlock(out_features)] # 上采样 for _ in range(2): out_features //= 2 model += [ nn.Upsample(scale_factor=2), nn.Conv2d(in_features, out_features, 3, stride=1, padding=1), nn.InstanceNorm2d(out_features), nn.ReLU(inplace=True) ] in_features = out_features # 输出层 model += [nn.ReflectionPad2d(3), nn.Conv2d(out_features, channels, 7), nn.Tanh()] self.model = nn.Sequential(*model) def forward(self, x): return self.model(x) StyleGAN StyleGAN通过风格控制生成高质量的人脸图像，具有出色的可控性和多样性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class StyleGAN_Generator(nn.Module): def __init__(self, latent_dim, n_mlp=8): super(StyleGAN_Generator, self).__init__() # 映射网络 layers = [] for i in range(n_mlp): layers.append(nn.Linear(latent_dim, latent_dim)) layers.append(nn.LeakyReLU(0.2)) self.mapping = nn.Sequential(*layers) # 合成网络 self.synthesis = self._build_synthesis_network(latent_dim) def _build_synthesis_network(self, latent_dim): # 这里简化了StyleGAN的合成网络结构 # 实际的StyleGAN结构更为复杂，包括AdaIN、噪声注入等 layers = nn.ModuleList() # 初始常数 self.constant_input = nn.Parameter(torch.randn(1, 512, 4, 4)) # 生成块 in_channels = 512 for i in range(8): # 8个上采样块 out_channels = min(512, 512 // (2 ** (i // 2))) layers.append(StyleGAN_Block(in_channels, out_channels, upsample=(i \u0026gt; 0))) in_channels = out_channels # 输出层 layers.append(nn.Conv2d(in_channels, 3, 1)) layers.append(nn.Tanh()) return nn.Sequential(*layers) def forward(self, z): # 通过映射网络 w = self.mapping(z) # 通过合成网络 x = self.synthesis(w) return x class StyleGAN_Block(nn.Module): def __init__(self, in_channels, out_channels, upsample=False): super(StyleGAN_Block, self).__init__() self.upsample = upsample if upsample: self.up = nn.Upsample(scale_factor=2, mode=\u0026#39;bilinear\u0026#39;, align_corners=False) self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1) self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1) self.activate = nn.LeakyReLU(0.2) def forward(self, x): if self.upsample: x = self.up(x) x = self.conv1(x) x = self.activate(x) x = self.conv2(x) x = self.activate(x) return x GAN在图像处理中的应用 图像生成 GAN可以生成各种类型的图像，从简单的人脸到复杂的场景。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 使用预训练的StyleGAN生成人脸 import torch from stylegan2_pytorch import Generator # 加载预训练模型 model = Generator(256, 512, 8).cuda() # 假设有预训练权重 model.load_state_dict(torch.load(\u0026#39;stylegan2-ffhq-config-f.pt\u0026#39;)) model.eval() # 生成随机潜在向量 z = torch.randn(1, 512).cuda() # 生成图像 with torch.no_grad(): img = model(z) 图像修复 GAN可以用于修复图像中的缺失部分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # 简化的图像修复模型 class ImageInpainting(nn.Module): def __init__(self): super(ImageInpainting, self).__init__() # 编码器 self.encoder = nn.Sequential( nn.Conv2d(4, 64, 7, stride=1, padding=3), # 4通道：RGB + mask nn.ReLU(inplace=True), nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True), nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 512, 3, stride=2, padding=1), nn.ReLU(inplace=True), ) # 中间层 self.middle = nn.Sequential( nn.Conv2d(512, 512, 3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(512, 512, 3, stride=1, padding=1), nn.ReLU(inplace=True), ) # 解码器 self.decoder = nn.Sequential( nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1), nn.ReLU(inplace=True), nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1), nn.ReLU(inplace=True), nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 3, 7, stride=1, padding=3), nn.Tanh(), ) def forward(self, x, mask): # 连接图像和掩码 x_masked = x * (1 - mask) input = torch.cat([x_masked, mask], dim=1) # 编码 x = self.encoder(input) # 中间处理 x = self.middle(x) # 解码 x = self.decoder(x) # 组合原始图像和生成部分 output = x * mask + x_masked return output 图像超分辨率 GAN可以用于将低分辨率图像转换为高分辨率图像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 # SRGAN生成器 class SRGAN_Generator(nn.Module): def __init__(self, scale_factor=4): super(SRGAN_Generator, self).__init__() # 初始卷积 self.conv1 = nn.Conv2d(3, 64, 9, stride=1, padding=4) self.relu = nn.ReLU(inplace=True) # 残差块 residual_blocks = [] for _ in range(16): residual_blocks.append(ResidualBlock(64)) self.residual_blocks = nn.Sequential(*residual_blocks) # 上采样 upsampling = [] for _ in range(int(math.log(scale_factor, 2))): upsampling.append(nn.Conv2d(64, 256, 3, stride=1, padding=1)) upsampling.append(nn.PixelShuffle(2)) upsampling.append(nn.ReLU(inplace=True)) self.upsampling = nn.Sequential(*upsampling) # 输出层 self.conv2 = nn.Conv2d(64, 3, 9, stride=1, padding=4) self.tanh = nn.Tanh() def forward(self, x): # 初始卷积 x = self.conv1(x) residual = x x = self.relu(x) # 残差块 x = self.residual_blocks(x) # 残差连接 x = x + residual # 上采样 x = self.upsampling(x) # 输出 x = self.conv2(x) x = self.tanh(x) return x class ResidualBlock(nn.Module): def __init__(self, channels): super(ResidualBlock, self).__init__() self.conv1 = nn.Conv2d(channels, channels, 3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(channels) self.relu = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(channels, channels, 3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(channels) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = out + residual return out 风格迁移 GAN可以实现从一种艺术风格到另一种风格的图像转换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 简化的风格迁移网络 class StyleTransfer(nn.Module): def __init__(self): super(StyleTransfer, self).__init__() # 编码器 self.encoder = nn.Sequential( nn.Conv2d(3, 32, 9, stride=1, padding=4), nn.InstanceNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.InstanceNorm2d(64), nn.ReLU(inplace=True), nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.InstanceNorm2d(128), nn.ReLU(inplace=True), ) # 残差块 residual_blocks = [] for _ in range(5): residual_blocks.append(ResidualBlock(128)) self.residual_blocks = nn.Sequential(*residual_blocks) # 解码器 self.decoder = nn.Sequential( nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.InstanceNorm2d(64), nn.ReLU(inplace=True), nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.InstanceNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(32, 3, 9, stride=1, padding=4), nn.Tanh(), ) def forward(self, x): # 编码 x = self.encoder(x) # 残差处理 x = self.residual_blocks(x) # 解码 x = self.decoder(x) return x 其他深度学习模型在图像处理中的应用 自编码器(Autoencoder) 自编码器是一种无监督学习模型，通过编码器将输入压缩为低维表示，再通过解码器重构原始输入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Autoencoder(nn.Module): def __init__(self, latent_dim): super(Autoencoder, self).__init__() # 编码器 self.encoder = nn.Sequential( nn.Conv2d(3, 32, 3, stride=2, padding=1), # 16x16 nn.ReLU(inplace=True), nn.Conv2d(32, 64, 3, stride=2, padding=1), # 8x8 nn.ReLU(inplace=True), nn.Conv2d(64, 128, 3, stride=2, padding=1), # 4x4 nn.ReLU(inplace=True), nn.Flatten(), nn.Linear(128 * 4 * 4, latent_dim), ) # 解码器 self.decoder = nn.Sequential( nn.Linear(latent_dim, 128 * 4 * 4), nn.Unflatten(1, (128, 4, 4)), nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), # 8x8 nn.ReLU(inplace=True), nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), # 16x16 nn.ReLU(inplace=True), nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1), # 32x32 nn.Sigmoid(), ) def forward(self, x): z = self.encoder(x) x_reconstructed = self.decoder(z) return x_reconstructed, z 变分自编码器(VAE) 变分自编码器是自编码器的概率版本，可以生成新的数据样本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class VAE(nn.Module): def __init__(self, latent_dim): super(VAE, self).__init__() # 编码器 self.encoder = nn.Sequential( nn.Conv2d(3, 32, 3, stride=2, padding=1), # 16x16 nn.ReLU(inplace=True), nn.Conv2d(32, 64, 3, stride=2, padding=1), # 8x8 nn.ReLU(inplace=True), nn.Conv2d(64, 128, 3, stride=2, padding=1), # 4x4 nn.ReLU(inplace=True), nn.Flatten(), ) # 均值和方差 self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim) self.fc_var = nn.Linear(128 * 4 * 4, latent_dim) # 解码器 self.decoder = nn.Sequential( nn.Linear(latent_dim, 128 * 4 * 4), nn.Unflatten(1, (128, 4, 4)), nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), # 8x8 nn.ReLU(inplace=True), nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), # 16x16 nn.ReLU(inplace=True), nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1), # 32x32 nn.Sigmoid(), ) def encode(self, x): h = self.encoder(x) mu = self.fc_mu(h) log_var = self.fc_var(h) return mu, log_var def reparameterize(self, mu, log_var): std = torch.exp(0.5 * log_var) eps = torch.randn_like(std) z = mu + eps * std return z def decode(self, z): return self.decoder(z) def forward(self, x): mu, log_var = self.encode(x) z = self.reparameterize(mu, log_var) x_reconstructed = self.decode(z) return x_reconstructed, mu, log_var 扩散模型(Diffusion Model) 扩散模型是近年来兴起的生成模型，通过逐步添加和去除噪声来生成图像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class DiffusionModel(nn.Module): def __init__(self, timesteps=1000): super(DiffusionModel, self).__init__() self.timesteps = timesteps # 噪声调度器 self.beta = torch.linspace(0.0001, 0.02, timesteps) self.alpha = 1. - self.beta self.alpha_hat = torch.cumprod(self.alpha, dim=0) # U-Net结构 self.unet = self._build_unet() def _build_unet(self): # 简化的U-Net结构 return nn.Sequential( # 下采样 nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(inplace=True), nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(inplace=True), # 中间层 nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True), # 上采样 nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1), nn.ReLU(inplace=True), nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 3, 3, padding=1), ) def forward(self, x, t): # 添加时间嵌入 t_emb = self._get_time_embedding(t, x.shape[0]) t_emb = t_emb.view(-1, 1, 1, 1).expand(-1, 3, x.shape[2], x.shape[3]) x = torch.cat([x, t_emb], dim=1) # 通过U-Net预测噪声 noise_pred = self.unet(x) return noise_pred def _get_time_embedding(self, t, batch_size): # 简化的时间嵌入 t = t.view(-1, 1) t = t.float() / self.timesteps t = t * 2 * math.pi sin_t = torch.sin(t) cos_t = torch.cos(t) t_emb = torch.cat([sin_t, cos_t], dim=1) t_emb = t_emb.repeat(1, 3) # 扩展到3通道 return t_emb def sample(self, x_shape): # 从纯噪声开始 x = torch.randn(x_shape) # 逐步去噪 for t in reversed(range(self.timesteps)): t_batch = torch.full((x_shape[0],), t, dtype=torch.long) noise_pred = self.forward(x, t_batch) # 计算去噪后的图像 alpha_t = self.alpha[t] alpha_hat_t = self.alpha_hat[t] beta_t = self.beta[t] if t \u0026gt; 0: noise = torch.randn_like(x) else: noise = torch.zeros_like(x) x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)) * noise_pred) + torch.sqrt(beta_t) * noise return x 视觉Transformer(ViT) 视觉Transformer将Transformer架构应用于图像处理任务，在许多任务上取得了与CNN相当甚至更好的性能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class PatchEmbed(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768): super(PatchEmbed, self).__init__() self.img_size = img_size self.patch_size = patch_size self.n_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): x = self.proj(x) # (B, embed_dim, n_patches ** 0.5, n_patches ** 0.5) x = x.flatten(2) # (B, embed_dim, n_patches) x = x.transpose(1, 2) # (B, n_patches, embed_dim) return x class Attention(nn.Module): def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.): super(Attention, self).__init__() self.n_heads = n_heads self.dim = dim self.head_dim = dim // n_heads self.scale = self.head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_p) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_p) def forward(self, x): n_samples, n_tokens, dim = x.shape qkv = self.qkv(x) # (n_samples, n_tokens, 3 * dim) qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim) qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_tokens, head_dim) q, k, v = qkv[0], qkv[1], qkv[2] k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_tokens) dp = (q @ k_t) * self.scale # (n_samples, n_heads, n_tokens, n_tokens) attn = dp.softmax(dim=-1) # (n_samples, n_heads, n_tokens, n_tokens) attn = self.attn_drop(attn) weighted_avg = attn @ v # (n_samples, n_heads, n_tokens, head_dim) weighted_avg = weighted_avg.transpose(1, 2) # (n_samples, n_tokens, n_heads, head_dim) weighted_avg = weighted_avg.flatten(2) # (n_samples, n_tokens, dim) x = self.proj(weighted_avg) x = self.proj_drop(x) return x class MLP(nn.Module): def __init__(self, in_features, hidden_features, out_features, p=0.): super(MLP, self).__init__() self.fc1 = nn.Linear(in_features, hidden_features) self.act = nn.GELU() self.fc2 = nn.Linear(hidden_features, out_features) self.drop = nn.Dropout(p) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.drop(x) x = self.fc2(x) x = self.drop(x) return x class Block(nn.Module): def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.): super(Block, self).__init__() self.norm1 = nn.LayerNorm(dim, eps=1e-6) self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p) self.norm2 = nn.LayerNorm(dim, eps=1e-6) hidden_features = int(dim * mlp_ratio) self.mlp = MLP(in_features=dim, hidden_features=hidden_features, out_features=dim, p=p) def forward(self, x): x = x + self.attn(self.norm1(x)) x = x + self.mlp(self.norm2(x)) return x class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_channels=3, n_classes=1000, embed_dim=768, depth=12, n_heads=12, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.): super(VisionTransformer, self).__init__() self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_channels=in_channels, embed_dim=embed_dim) self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)) self.pos_drop = nn.Dropout(p=p) self.blocks = nn.ModuleList([ Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p) for _ in range(depth) ]) self.norm = nn.LayerNorm(embed_dim, eps=1e-6) self.head = nn.Linear(embed_dim, n_classes) def forward(self, x): n_samples = x.shape[0] x = self.patch_embed(x) cls_token = self.cls_token.expand(n_samples, -1, -1) x = torch.cat((cls_token, x), dim=1) x = x + self.pos_embed x = self.pos_drop(x) for block in self.blocks: x = block(x) x = self.norm(x) cls_token_final = x[:, 0] x = self.head(cls_token_final) return x 深度学习图像处理的挑战与未来方向 当前挑战 数据需求：深度学习模型通常需要大量标注数据，获取成本高。 计算资源：训练大型模型需要强大的计算资源，限制了应用范围。 可解释性：深度学习模型通常被视为\u0026quot;黑盒\u0026quot;，难以解释其决策过程。 泛化能力：模型在训练数据分布外表现不佳，鲁棒性有待提高。 领域适应：将模型从一个领域迁移到另一个领域仍然具有挑战性。 未来方向 自监督学习：减少对标注数据的依赖，从未标注数据中学习。 小样本学习：使模型能够从少量样本中学习。 多模态学习：结合图像、文本、音频等多种模态的信息。 神经架构搜索：自动设计最优的网络结构。 模型压缩与加速：使模型能够在资源受限的设备上运行。 可解释AI：提高模型的透明度和可解释性。 鲁棒性增强：提高模型对对抗样本和分布外数据的鲁棒性。 总结 深度学习技术，特别是CNN和GAN，已经彻底改变了图像处理领域。从图像分类、目标检测到图像生成和风格迁移，深度学习模型在各种任务中都取得了令人瞩目的成果。\nCNN通过其局部连接和权值共享的特性，有效地提取图像的层次特征，成为图像处理的基础架构。GAN通过生成器和判别器的对抗训练，能够生成逼真的图像，为图像生成和转换任务提供了强大的工具。\n除了CNN和GAN，自编码器、变分自编码器、扩散模型和视觉Transformer等模型也在图像处理中发挥着重要作用，不断推动着该领域的发展。\n尽管深度学习在图像处理中取得了巨大成功，但仍面临数据需求、计算资源、可解释性等挑战。未来，自监督学习、小样本学习、多模态学习等方向将引领图像处理领域的进一步发展。\n作为图像算法工程师，了解和掌握这些深度学习模型对于解决实际问题至关重要。通过不断学习和实践，我们可以更好地应用这些技术，推动图像处理和计算机视觉领域的创新和发展。\n","permalink":"http://localhost:13141/posts/deep-learning-image-processing/","summary":"\u003ch1 id=\"深度学习在图像处理中的应用从cnn到gan\"\u003e深度学习在图像处理中的应用：从CNN到GAN\u003c/h1\u003e\n\u003cp\u003e深度学习技术的快速发展为图像处理领域带来了革命性的变化。从图像分类到目标检测，从图像分割到图像生成，深度学习模型在各项任务中都取得了突破性的进展。本文将探讨深度学习在图像处理中的主要应用，重点关注卷积神经网络(CNN)和生成对抗网络(GAN)等核心模型。\u003c/p\u003e","title":"深度学习在图像处理中的应用"},{"content":"算法优化：从理论到实践 在软件开发中，算法优化是提升程序性能的关键环节。一个高效的算法可以在相同硬件条件下显著提高程序的运行速度，减少资源消耗。本文将深入探讨算法优化的各种技术和方法，从理论分析到实际应用，帮助开发者全面提升代码性能。\n算法复杂度分析 时间复杂度 时间复杂度是衡量算法执行时间随输入规模增长而增长的速率。常见的时间复杂度从低到高依次为：\nO(1) - 常数时间 常数时间算法的执行时间与输入规模无关，是最理想的复杂度。\n1 2 3 # 示例：获取数组第一个元素 def get_first_element(arr): return arr[0] # 无论数组多大，执行时间相同 O(log n) - 对数时间 对数时间算法的执行时间随输入规模的对数增长，常见于分治算法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026#34;\u0026#34;\u0026#34; 示例：二分查找 参数：arr (List[int])，target (int) 返回：目标索引或-1 \u0026#34;\u0026#34;\u0026#34; def binary_search(arr, target): left, right = 0, len(arr) - 1 while left \u0026lt;= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] \u0026lt; target: left = mid + 1 else: right = mid - 1 return -1 O(n) - 线性时间 线性时间算法的执行时间与输入规模成线性关系。\n1 2 3 4 5 6 7 # 示例：查找数组中的最大值 def find_max(arr): max_val = arr[0] for val in arr: if val \u0026gt; max_val: max_val = val return max_val O(n log n) - 线性对数时间 线性对数时间算法常见于高效的排序算法，如快速排序、归并排序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 示例：归并排序 def merge_sort(arr): if len(arr) \u0026lt;= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) right = merge_sort(arr[mid:]) return merge(left, right) def merge(left, right): result = [] i = j = 0 while i \u0026lt; len(left) and j \u0026lt; len(right): if left[i] \u0026lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result.extend(left[i:]) result.extend(right[j:]) return result O(n²) - 平方时间 平方时间算法的执行时间与输入规模的平方成正比，常见于简单的排序算法和嵌套循环。\n1 2 3 4 5 6 7 8 # 示例：冒泡排序 def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n - i - 1): if arr[j] \u0026gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] return arr O(2ⁿ) - 指数时间 指数时间算法的执行时间随输入规模指数增长，通常用于解决NP难问题。\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;\u0026#34;\u0026#34; 示例：递归计算斐波那契数列（低效版本） 参数：n (int) 返回：第n项斐波那契数 \u0026#34;\u0026#34;\u0026#34; def fibonacci(n): if not isinstance(n, int) or n \u0026lt; 0: raise ValueError(\u0026#34;n必须为非负整数\u0026#34;) if n \u0026lt;= 1: return n return fibonacci(n - 1) + fibonacci(n - 2) O(n!) - 阶乘时间 阶乘时间算法的执行时间随输入规模的阶乘增长，是最差的复杂度，常见于暴力搜索所有排列组合。\n1 2 3 4 5 6 7 8 9 10 11 # 示例：生成所有排列 def permutations(arr): if len(arr) \u0026lt;= 1: return [arr] result = [] for i in range(len(arr)): rest = arr[:i] + arr[i+1:] for p in permutations(rest): result.append([arr[i]] + p) return result 空间复杂度 空间复杂度衡量算法执行过程中所需额外空间随输入规模增长的速率。\nO(1) - 常数空间 常数空间算法使用的额外空间与输入规模无关。\n1 2 3 # 示例：原地交换数组元素 def swap_elements(arr, i, j): arr[i], arr[j] = arr[j], arr[i] # 不需要额外空间 O(n) - 线性空间 线性空间算法使用的额外空间与输入规模成线性关系。\n1 2 3 # 示例：复制数组 def copy_array(arr): return arr.copy() # 需要与原数组大小相同的额外空间 O(n²) - 平方空间 平方空间算法使用的额外空间与输入规模的平方成正比。\n1 2 3 # 示例：创建二维数组 def create_2d_array(n): return [[0 for _ in range(n)] for _ in range(n)] # 需要n²的额外空间 复杂度分析技巧 循环分析 对于循环结构，复杂度通常由循环次数和循环体内的操作决定。\n1 2 3 4 5 6 7 8 9 10 # O(n) - 单层循环 def example1(n): for i in range(n): # 循环n次 print(i) # O(1)操作 # O(n²) - 嵌套循环 def example2(n): for i in range(n): # 外层循环n次 for j in range(n): # 内层循环n次 print(i, j) # O(1)操作 递归分析 对于递归算法，可以使用递归树或主定理(Master Theorem)来分析复杂度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 递归树分析：归并排序 # T(n) = 2T(n/2) + O(n) # 每层总复杂度为O(n)，共有log n层，因此总复杂度为O(n log n) def merge_sort(arr): if len(arr) \u0026lt;= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) # T(n/2) right = merge_sort(arr[mid:]) # T(n/2) return merge(left, right) # O(n) 均摊分析 均摊分析用于计算一系列操作的平均复杂度，即使某些操作可能很耗时。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 动态数组的均摊分析 # 虽然偶尔需要O(n)时间扩容，但n次append操作的总时间为O(n) # 因此每次append的均摊时间为O(1) class DynamicArray: def __init__(self): self.capacity = 1 self.size = 0 self.array = [None] * self.capacity def append(self, item): if self.size == self.capacity: self._resize(2 * self.capacity) # O(n)操作，但不频繁 self.array[self.size] = item self.size += 1 def _resize(self, new_capacity): new_array = [None] * new_capacity for i in range(self.size): new_array[i] = self.array[i] self.array = new_array self.capacity = new_capacity 算法优化策略 时间优化策略 选择合适的算法和数据结构 选择合适的算法和数据结构是优化的第一步。例如，对于频繁查找操作，哈希表(O(1))比数组(O(n))更高效。\n1 2 3 4 5 6 7 8 9 10 # 使用哈希表优化查找 def find_duplicates(arr): seen = set() duplicates = [] for item in arr: if item in seen: # O(1)查找 duplicates.append(item) else: seen.add(item) return duplicates 预计算和缓存 对于重复计算，可以使用预计算或缓存技术避免重复工作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 使用缓存优化斐波那契数列计算 \u0026#34;\u0026#34;\u0026#34; 使用缓存优化斐波那契数列计算 参数：n (int), cache (dict) 返回：第n项斐波那契数 \u0026#34;\u0026#34;\u0026#34; def fibonacci(n, cache={}): if not isinstance(n, int) or n \u0026lt; 0: raise ValueError(\u0026#34;n必须为非负整数\u0026#34;) if n in cache: return cache[n] if n \u0026lt;= 1: return n result = fibonacci(n - 1, cache) + fibonacci(n - 2, cache) cache[n] = result return result 位运算优化 位运算通常比算术运算更快，可以用于某些特定场景的优化。\n1 2 3 4 5 6 7 8 9 10 # 使用位运算判断奇偶 def is_even(n): return (n \u0026amp; 1) == 0 # 比n % 2 == 0更快 # 使用位运算交换变量 def swap(a, b): a = a ^ b b = a ^ b a = a ^ b return a, b 并行计算 对于可以并行处理的问题，可以使用多线程或多进程加速。\n1 2 3 4 5 6 7 8 9 10 11 12 # 使用多线程并行处理 import concurrent.futures def process_data(data): # 处理数据的函数，返回处理结果 result = ... # 根据实际需求处理 return result def parallel_process(data_list, num_workers=4): with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor: results = list(executor.map(process_data, data_list)) return results 空间优化策略 原地算法 原地算法不需要额外的存储空间或只需要常数级别的额外空间。\n1 2 3 4 5 6 7 8 # 原地反转数组 def reverse_array(arr): left, right = 0, len(arr) - 1 while left \u0026lt; right: arr[left], arr[right] = arr[right], arr[left] left += 1 right -= 1 return arr 数据压缩 对于大规模数据，可以使用压缩技术减少存储需求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 使用稀疏矩阵表示优化存储 class SparseMatrix: def __init__(self, rows, cols): self.rows = rows self.cols = cols self.data = {} # 只存储非零元素 def set(self, i, j, value): if value != 0: self.data[(i, j)] = value elif (i, j) in self.data: del self.data[(i, j)] def get(self, i, j): return self.data.get((i, j), 0) 惰性计算 惰性计算只在需要时才计算结果，可以节省不必要的计算和存储。\n1 2 3 4 5 6 7 8 9 10 11 # 惰性计算斐波那契数列 def lazy_fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a + b # 使用生成器 fib = lazy_fibonacci() for _ in range(10): print(next(fib)) 时空权衡 有时可以通过增加空间使用来减少时间复杂度，或者通过增加时间复杂度来减少空间使用。\n空间换时间 使用额外的空间来存储中间结果，避免重复计算。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 使用动态规划优化最长公共子序列 def longest_common_subsequence(text1, text2): m, n = len(text1), len(text2) # 创建二维数组存储中间结果 dp = [[0] * (n + 1) for _ in range(m + 1)] for i in range(1, m + 1): for j in range(1, n + 1): if text1[i - 1] == text2[j - 1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) return dp[m][n] 时间换空间 通过增加计算时间来减少空间使用。\n1 2 3 4 5 6 7 8 9 10 11 # 使用滚动数组优化空间复杂度 def fibonacci_with_rolling_array(n): if n \u0026lt;= 1: return n # 只保存最近的两个值 a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b 常见算法优化案例 排序算法优化 快速排序优化 快速排序的平均时间复杂度为O(n log n)，但在最坏情况下会退化到O(n²)。以下是几种优化方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def optimized_quick_sort(arr): # 使用三数取中法选择基准，避免最坏情况 def median_of_three(left, right): mid = (left + right) // 2 if arr[left] \u0026gt; arr[mid]: arr[left], arr[mid] = arr[mid], arr[left] if arr[left] \u0026gt; arr[right]: arr[left], arr[right] = arr[right], arr[left] if arr[mid] \u0026gt; arr[right]: arr[mid], arr[right] = arr[right], arr[mid] return mid def partition(left, right): # 选择基准 pivot_idx = median_of_three(left, right) pivot = arr[pivot_idx] # 将基准移到最右边 arr[pivot_idx], arr[right] = arr[right], arr[pivot_idx] i = left for j in range(left, right): if arr[j] \u0026lt;= pivot: arr[i], arr[j] = arr[j], arr[i] i += 1 # 将基准移到正确位置 arr[i], arr[right] = arr[right], arr[i] return i def sort(left, right): # 小数组使用插入排序 if right - left + 1 \u0026lt;= 20: insertion_sort(arr, left, right) return if left \u0026lt; right: pivot_idx = partition(left, right) sort(left, pivot_idx - 1) sort(pivot_idx + 1, right) def insertion_sort(arr, left, right): for i in range(left + 1, right + 1): key = arr[i] j = i - 1 while j \u0026gt;= left and arr[j] \u0026gt; key: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key sort(0, len(arr) - 1) return arr 计数排序优化 计数排序是一种非比较排序算法，适用于整数且范围不大的情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def counting_sort(arr, max_val=None): if not arr: return arr if max_val is None: max_val = max(arr) # 创建计数数组 count = [0] * (max_val + 1) # 统计每个元素的出现次数 for num in arr: count[num] += 1 # 计算累积计数 for i in range(1, len(count)): count[i] += count[i - 1] # 构建排序结果 result = [0] * len(arr) for num in reversed(arr): result[count[num] - 1] = num count[num] -= 1 return result 搜索算法优化 二分查找优化 二分查找是一种高效的搜索算法，时间复杂度为O(log n)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def binary_search_optimized(arr, target): left, right = 0, len(arr) - 1 while left \u0026lt;= right: # 防止整数溢出 mid = left + (right - left) // 2 if arr[mid] == target: return mid elif arr[mid] \u0026lt; target: left = mid + 1 else: right = mid - 1 return -1 跳表搜索优化 跳表是一种概率数据结构，允许快速搜索，类似于平衡树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import random class SkipNode: def __init__(self, val=None, level=0): self.val = val self.next = [None] * level class SkipList: def __init__(self, max_level=16, p=0.5): self.max_level = max_level self.p = p self.level = 1 self.head = SkipNode(None, max_level) def random_level(self): level = 1 while random.random() \u0026lt; self.p and level \u0026lt; self.max_level: level += 1 return level def insert(self, val): update = [None] * self.max_level current = self.head # 找到插入位置 for i in range(self.level - 1, -1, -1): while current.next[i] and current.next[i].val \u0026lt; val: current = current.next[i] update[i] = current # 创建新节点 node_level = self.random_level() if node_level \u0026gt; self.level: for i in range(self.level, node_level): update[i] = self.head self.level = node_level # 插入新节点 new_node = SkipNode(val, node_level) for i in range(node_level): new_node.next[i] = update[i].next[i] update[i].next[i] = new_node def search(self, val): current = self.head for i in range(self.level - 1, -1, -1): while current.next[i] and current.next[i].val \u0026lt; val: current = current.next[i] current = current.next[0] if current and current.val == val: return True return False 图算法优化 Dijkstra算法优化 Dijkstra算法用于寻找单源最短路径，可以使用优先队列优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import heapq def dijkstra_optimized(graph, start): n = len(graph) dist = [float(\u0026#39;inf\u0026#39;)] * n dist[start] = 0 # 使用优先队列 pq = [(0, start)] while pq: current_dist, u = heapq.heappop(pq) # 如果已经找到更短路径，跳过 if current_dist \u0026gt; dist[u]: continue for v, weight in graph[u]: distance = current_dist + weight if distance \u0026lt; dist[v]: dist[v] = distance heapq.heappush(pq, (distance, v)) return dist A*算法优化 A*算法是一种启发式搜索算法，常用于路径规划。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import heapq def a_star_search(graph, start, goal, heuristic): # 优先队列：(f_score, node) open_set = [(0, start)] # 从起点到每个节点的实际代价 g_score = {node: float(\u0026#39;inf\u0026#39;) for node in graph} g_score[start] = 0 # 从起点经过每个节点到终点的估计代价 f_score = {node: float(\u0026#39;inf\u0026#39;) for node in graph} f_score[start] = heuristic(start, goal) # 记录路径 came_from = {} while open_set: current_f, current = heapq.heappop(open_set) if current == goal: # 重建路径 path = [current] while current in came_from: current = came_from[current] path.append(current) return path[::-1] for neighbor in graph[current]: # 计算从起点到邻居的临时g_score tentative_g_score = g_score[current] + graph[current][neighbor] if tentative_g_score \u0026lt; g_score[neighbor]: # 找到更好的路径 came_from[neighbor] = current g_score[neighbor] = tentative_g_score f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, goal) heapq.heappush(open_set, (f_score[neighbor], neighbor)) return None # 没有找到路径 动态规划优化 状态压缩 对于某些动态规划问题，可以使用位运算进行状态压缩，减少空间使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 旅行商问题(TSP)的状态压缩优化 def tsp_dp(distances): n = len(distances) # dp[mask][i]表示访问过mask中的城市，最后停留在城市i的最短距离 dp = [[float(\u0026#39;inf\u0026#39;)] * n for _ in range(1 \u0026lt;\u0026lt; n)] dp[1][0] = 0 # 从城市0开始 for mask in range(1 \u0026lt;\u0026lt; n): for i in range(n): if mask \u0026amp; (1 \u0026lt;\u0026lt; i): # 如果城市i在mask中 for j in range(n): if not mask \u0026amp; (1 \u0026lt;\u0026lt; j): # 如果城市j不在mask中 new_mask = mask | (1 \u0026lt;\u0026lt; j) dp[new_mask][j] = min(dp[new_mask][j], dp[mask][i] + distances[i][j]) # 计算回到起点的最短距离 final_mask = (1 \u0026lt;\u0026lt; n) - 1 min_distance = float(\u0026#39;inf\u0026#39;) for i in range(1, n): min_distance = min(min_distance, dp[final_mask][i] + distances[i][0]) return min_distance 滚动数组优化 对于某些动态规划问题，可以使用滚动数组优化空间复杂度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 最长公共子序列的滚动数组优化 def lcs_rolling_array(text1, text2): m, n = len(text1), len(text2) # 使用两行数组代替完整的二维数组 prev = [0] * (n + 1) curr = [0] * (n + 1) for i in range(1, m + 1): for j in range(1, n + 1): if text1[i - 1] == text2[j - 1]: curr[j] = prev[j - 1] + 1 else: curr[j] = max(prev[j], curr[j - 1]) # 滚动数组 prev, curr = curr, prev curr = [0] * (n + 1) return prev[n] 实际应用案例分析 图像处理中的优化 卷积运算优化 卷积运算是图像处理中的基本操作，可以通过多种方式优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np def naive_convolution(image, kernel): # 原始卷积实现 height, width = image.shape k_height, k_width = kernel.shape output = np.zeros((height - k_height + 1, width - k_width + 1)) for i in range(output.shape[0]): for j in range(output.shape[1]): output[i, j] = np.sum(image[i:i+k_height, j:j+k_width] * kernel) return output def optimized_convolution(image, kernel): # 使用FFT加速卷积 from scipy.signal import fftconvolve return fftconvolve(image, kernel, mode=\u0026#39;valid\u0026#39;) def separable_convolution(image, kernel): # 可分离卷积优化 # 如果kernel可以分离为水平和垂直两个一维核 # 例如：kernel = h_kernel * v_kernel^T # 假设kernel是可分离的 u, s, vh = np.linalg.svd(kernel) h_kernel = u[:, 0] * np.sqrt(s[0]) v_kernel = vh[0, :] * np.sqrt(s[0]) # 先进行水平卷积 temp = np.zeros_like(image) for i in range(image.shape[0]): temp[i, :] = np.convolve(image[i, :], h_kernel, mode=\u0026#39;valid\u0026#39;) # 再进行垂直卷积 output = np.zeros((temp.shape[0] - len(v_kernel) + 1, temp.shape[1])) for j in range(temp.shape[1]): output[:, j] = np.convolve(temp[:, j], v_kernel, mode=\u0026#39;valid\u0026#39;) return output 图像金字塔优化 图像金字塔是一种多尺度表示方法，可以用于加速图像处理算法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def build_gaussian_pyramid(image, levels): pyramid = [image] for _ in range(levels - 1): # 下采样 image = cv2.pyrDown(image) pyramid.append(image) return pyramid def process_with_pyramid(image, process_func, levels=4): # 构建金字塔 pyramid = build_gaussian_pyramid(image, levels) # 从最粗级别开始处理 result = process_func(pyramid[-1]) # 逐级上采样并细化 for i in range(levels - 2, -1, -1): # 上采样结果 result = cv2.pyrUp(result) # 调整大小以匹配当前级别 result = cv2.resize(result, (pyramid[i].shape[1], pyramid[i].shape[0])) # 与当前级别结合 result = process_func(pyramid[i], result) return result 机器学习中的优化 梯度下降优化 梯度下降是机器学习中最常用的优化算法之一，有多种变体。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 import numpy as np def gradient_descent(X, y, learning_rate=0.01, epochs=1000): m, n = X.shape theta = np.zeros(n) for _ in range(epochs): # 计算预测值 predictions = X.dot(theta) # 计算误差 error = predictions - y # 计算梯度 gradient = X.T.dot(error) / m # 更新参数 theta -= learning_rate * gradient return theta def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100): m, n = X.shape theta = np.zeros(n) for _ in range(epochs): for i in range(m): # 随机选择一个样本 xi = X[i:i+1] yi = y[i:i+1] # 计算预测值 prediction = xi.dot(theta) # 计算误差 error = prediction - yi # 计算梯度 gradient = xi.T.dot(error) # 更新参数 theta -= learning_rate * gradient return theta def mini_batch_gradient_descent(X, y, batch_size=32, learning_rate=0.01, epochs=100): m, n = X.shape theta = np.zeros(n) for _ in range(epochs): # 随机打乱数据 indices = np.random.permutation(m) X_shuffled = X[indices] y_shuffled = y[indices] # 分批处理 for i in range(0, m, batch_size): X_batch = X_shuffled[i:i+batch_size] y_batch = y_shuffled[i:i+batch_size] # 计算预测值 predictions = X_batch.dot(theta) # 计算误差 error = predictions - y_batch # 计算梯度 gradient = X_batch.T.dot(error) / len(X_batch) # 更新参数 theta -= learning_rate * gradient return theta def momentum_gradient_descent(X, y, learning_rate=0.01, momentum=0.9, epochs=1000): m, n = X.shape theta = np.zeros(n) velocity = np.zeros(n) for _ in range(epochs): # 计算预测值 predictions = X.dot(theta) # 计算误差 error = predictions - y # 计算梯度 gradient = X.T.dot(error) / m # 更新速度 velocity = momentum * velocity - learning_rate * gradient # 更新参数 theta += velocity return theta 矩阵运算优化 在机器学习中，矩阵运算是核心操作，可以通过多种方式优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import numpy as np def naive_matrix_multiply(A, B): # 原始矩阵乘法实现 m, n = A.shape n2, p = B.shape assert n == n2, \u0026#34;矩阵维度不匹配\u0026#34; C = np.zeros((m, p)) for i in range(m): for j in range(p): for k in range(n): C[i, j] += A[i, k] * B[k, j] return C def blocked_matrix_multiply(A, B, block_size=32): # 分块矩阵乘法优化 m, n = A.shape n2, p = B.shape assert n == n2, \u0026#34;矩阵维度不匹配\u0026#34; C = np.zeros((m, p)) for i in range(0, m, block_size): for j in range(0, p, block_size): for k in range(0, n, block_size): # 处理当前块 for ii in range(i, min(i + block_size, m)): for jj in range(j, min(j + block_size, p)): for kk in range(k, min(k + block_size, n)): C[ii, jj] += A[ii, kk] * B[kk, jj] return C def vectorized_matrix_multiply(A, B): # 向量化矩阵乘法（使用NumPy内置函数） return np.dot(A, B) def parallel_matrix_multiply(A, B): # 并行矩阵乘法 from concurrent.futures import ThreadPoolExecutor m, n = A.shape n2, p = B.shape assert n == n2, \u0026#34;矩阵维度不匹配\u0026#34; C = np.zeros((m, p)) def compute_row(i): for j in range(p): for k in range(n): C[i, j] += A[i, k] * B[k, j] with ThreadPoolExecutor() as executor: executor.map(compute_row, range(m)) return C 数据库查询优化 索引优化 索引是数据库查询优化的关键，可以显著提高查询速度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 # 简单的B树索引实现 class BTreeNode: def __init__(self, leaf=False): self.keys = [] self.children = [] self.leaf = leaf class BTree: def __init__(self, t): self.root = BTreeNode(leaf=True) self.t = t # 最小度数 def search(self, key, node=None): if node is None: node = self.root i = 0 while i \u0026lt; len(node.keys) and key \u0026gt; node.keys[i]: i += 1 if i \u0026lt; len(node.keys) and key == node.keys[i]: return True # 找到键 if node.leaf: return False # 未找到键 return self.search(key, node.children[i]) def insert(self, key): root = self.root if len(root.keys) == (2 * self.t) - 1: # 根节点已满，创建新根节点 new_root = BTreeNode() new_root.children.append(self.root) self.root = new_root self._split_child(new_root, 0) self._insert_nonfull(new_root, key) else: self._insert_nonfull(root, key) def _split_child(self, parent, index): t = self.t y = parent.children[index] z = BTreeNode(leaf=y.leaf) # 将y的中间键提升到父节点 parent.keys.insert(index, y.keys[t-1]) # 将y的后半部分键复制到z z.keys = y.keys[t:(2*t-1)] # 如果y不是叶子节点，复制子节点 if not y.leaf: z.children = y.children[t:(2*t)] # 更新y的键和子节点 y.keys = y.keys[0:(t-1)] y.children = y.children[0:t] # 将z插入父节点的子节点列表 parent.children.insert(index + 1, z) def _insert_nonfull(self, node, key): i = len(node.keys) - 1 if node.leaf: # 在叶子节点中插入键 node.keys.append(0) while i \u0026gt;= 0 and key \u0026lt; node.keys[i]: node.keys[i+1] = node.keys[i] i -= 1 node.keys[i+1] = key else: # 找到合适的子节点 while i \u0026gt;= 0 and key \u0026lt; node.keys[i]: i -= 1 i += 1 # 如果子节点已满，先分裂 if len(node.children[i].keys) == (2 * self.t) - 1: self._split_child(node, i) if key \u0026gt; node.keys[i]: i += 1 self._insert_nonfull(node.children[i], key) 查询计划优化 查询计划优化是数据库系统的核心功能，可以通过多种策略优化查询执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class QueryOptimizer: def __init__(self, database): self.database = database def optimize_query(self, query): # 解析查询 parsed_query = self._parse_query(query) # 生成可能的执行计划 plans = self._generate_execution_plans(parsed_query) # 评估每个计划的成本 plan_costs = [self._estimate_cost(plan) for plan in plans] # 选择成本最低的计划 best_plan = plans[plan_costs.index(min(plan_costs))] return best_plan def _parse_query(self, query): # 简化的查询解析 # 实际实现会更复杂 return { \u0026#39;tables\u0026#39;: query.get(\u0026#39;tables\u0026#39;, []), \u0026#39;conditions\u0026#39;: query.get(\u0026#39;conditions\u0026#39;, []), \u0026#39;projections\u0026#39;: query.get(\u0026#39;projections\u0026#39;, []), \u0026#39;order_by\u0026#39;: query.get(\u0026#39;order_by\u0026#39;, []), \u0026#39;limit\u0026#39;: query.get(\u0026#39;limit\u0026#39;, None) } def _generate_execution_plans(self, parsed_query): # 生成可能的执行计划 plans = [] # 简单实现：只考虑表连接顺序 tables = parsed_query[\u0026#39;tables\u0026#39;] # 生成所有可能的表连接顺序 from itertools import permutations for table_order in permutations(tables): plan = { \u0026#39;table_order\u0026#39;: table_order, \u0026#39;join_method\u0026#39;: \u0026#39;nested_loop\u0026#39;, # 可以是nested_loop, hash_join, merge_join \u0026#39;access_method\u0026#39;: {table: \u0026#39;index_scan\u0026#39; for table in tables}, # 可以是full_scan, index_scan \u0026#39;conditions\u0026#39;: parsed_query[\u0026#39;conditions\u0026#39;], \u0026#39;projections\u0026#39;: parsed_query[\u0026#39;projections\u0026#39;], \u0026#39;order_by\u0026#39;: parsed_query[\u0026#39;order_by\u0026#39;], \u0026#39;limit\u0026#39;: parsed_query[\u0026#39;limit\u0026#39;] } plans.append(plan) return plans def _estimate_cost(self, plan): # 估计执行计划的成本 cost = 0 # 估计表访问成本 for table in plan[\u0026#39;table_order\u0026#39;]: access_method = plan[\u0026#39;access_method\u0026#39;][table] table_stats = self.database.get_table_stats(table) if access_method == \u0026#39;full_scan\u0026#39;: cost += table_stats[\u0026#39;row_count\u0026#39;] elif access_method == \u0026#39;index_scan\u0026#39;: # 假设索引可以过滤掉90%的数据 cost += table_stats[\u0026#39;row_count\u0026#39;] * 0.1 # 估计连接成本 for i in range(len(plan[\u0026#39;table_order\u0026#39;]) - 1): join_method = plan[\u0026#39;join_method\u0026#39;] if join_method == \u0026#39;nested_loop\u0026#39;: # 嵌套循环连接的成本 left_table = plan[\u0026#39;table_order\u0026#39;][i] right_table = plan[\u0026#39;table_order\u0026#39;][i+1] left_stats = self.database.get_table_stats(left_table) right_stats = self.database.get_table_stats(right_table) cost += left_stats[\u0026#39;row_count\u0026#39;] * right_stats[\u0026#39;row_count\u0026#39;] elif join_method == \u0026#39;hash_join\u0026#39;: # 哈希连接的成本 left_table = plan[\u0026#39;table_order\u0026#39;][i] right_table = plan[\u0026#39;table_order\u0026#39;][i+1] left_stats = self.database.get_table_stats(left_table) right_stats = self.database.get_table_stats(right_table) cost += left_stats[\u0026#39;row_count\u0026#39;] + right_stats[\u0026#39;row_count\u0026#39;] elif join_method == \u0026#39;merge_join\u0026#39;: # 合并连接的成本 left_table = plan[\u0026#39;table_order\u0026#39;][i] right_table = plan[\u0026#39;table_order\u0026#39;][i+1] left_stats = self.database.get_table_stats(left_table) right_stats = self.database.get_table_stats(right_table) cost += left_stats[\u0026#39;row_count\u0026#39;] + right_stats[\u0026#39;row_count\u0026#39;] # 估计排序成本 if plan[\u0026#39;order_by\u0026#39;]: # 假设排序成本为n log n result_size = cost # 简化假设 cost += result_size * np.log2(result_size) return cost 性能分析工具 时间分析工具 Python中的时间分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import time import timeit import cProfile import pstats def time_function(func, *args, **kwargs): # 简单的时间测量 start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;函数 {func.__name__} 执行时间: {end_time - start_time:.6f} 秒\u0026#34;) return result def benchmark_function(func, *args, **kwargs): # 使用timeit进行更精确的基准测试 import functools wrapped = functools.partial(func, *args, **kwargs) time_taken = timeit.timeit(wrapped, number=1000) print(f\u0026#34;函数 {func.__name__} 平均执行时间: {time_taken/1000:.6f} 秒\u0026#34;) return func(*args, **kwargs) def profile_function(func, *args, **kwargs): # 使用cProfile进行详细性能分析 profiler = cProfile.Profile() profiler.enable() result = func(*args, **kwargs) profiler.disable() stats = pstats.Stats(profiler).sort_stats(\u0026#39;cumulative\u0026#39;) stats.print_stats() return result 内存分析工具 Python中的内存分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import sys import tracemalloc import objgraph def get_object_size(obj): # 获取对象的内存大小 return sys.getsizeof(obj) def trace_memory(func, *args, **kwargs): # 跟踪内存使用情况 tracemalloc.start() result = func(*args, **kwargs) snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\u0026#39;lineno\u0026#39;) print(\u0026#34;[ 内存使用最多的代码行 ]\u0026#34;) for stat in top_stats[:10]: print(stat) tracemalloc.stop() return result def analyze_object_growth(func, *args, **kwargs): # 分析对象增长情况 objgraph.show_growth() result = func(*args, **kwargs) objgraph.show_growth() return result 可视化分析工具 使用matplotlib可视化性能数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import matplotlib.pyplot as plt import numpy as np def plot_time_complexity(algorithms, input_sizes, title=\u0026#34;时间复杂度比较\u0026#34;): # 绘制算法时间复杂度比较图 plt.figure(figsize=(10, 6)) for name, func in algorithms.items(): times = [] for size in input_sizes: # 生成测试数据 test_data = generate_test_data(size) # 测量执行时间 start_time = time.time() func(test_data) end_time = time.time() times.append(end_time - start_time) plt.plot(input_sizes, times, label=name, marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;输入大小\u0026#39;) plt.ylabel(\u0026#39;执行时间 (秒)\u0026#39;) plt.title(title) plt.legend() plt.grid(True) plt.show() def generate_test_data(size): # 生成测试数据 return np.random.rand(size) def plot_memory_usage(func, input_sizes, title=\u0026#34;内存使用情况\u0026#34;): # 绘制函数内存使用情况图 memory_usage = [] for size in input_sizes: # 生成测试数据 test_data = generate_test_data(size) # 测量内存使用 tracemalloc.start() func(test_data) snapshot = tracemalloc.take_snapshot() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() memory_usage.append(peak / (1024 * 1024)) # 转换为MB plt.figure(figsize=(10, 6)) plt.plot(input_sizes, memory_usage, marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;输入大小\u0026#39;) plt.ylabel(\u0026#39;内存使用 (MB)\u0026#39;) plt.title(title) plt.grid(True) plt.show() 总结 算法优化是提升软件性能的关键环节。本文从算法复杂度分析开始，介绍了时间复杂度和空间复杂度的概念及分析方法，然后详细探讨了各种优化策略，包括时间优化、空间优化和时空权衡。\n通过常见算法优化案例，如排序算法、搜索算法、图算法和动态规划的优化，我们了解了如何将理论应用到实践中。实际应用案例分析展示了算法优化在图像处理、机器学习和数据库查询等领域的具体应用。\n最后，我们介绍了各种性能分析工具，帮助开发者识别性能瓶颈并进行针对性优化。\n算法优化是一个持续学习和实践的过程。随着技术的发展，新的优化方法和工具不断涌现。掌握这些优化技巧，不仅能够提高代码性能，还能培养系统思维和问题解决能力，为成为一名优秀的软件工程师奠定基础。\n希望本文能够帮助读者深入理解算法优化的原理和方法，并在实际开发中灵活应用，创造出更高效、更优雅的代码。\n","permalink":"http://localhost:13141/posts/algorithm-optimization/","summary":"\u003ch1 id=\"算法优化从理论到实践\"\u003e算法优化：从理论到实践\u003c/h1\u003e\n\u003cp\u003e在软件开发中，算法优化是提升程序性能的关键环节。一个高效的算法可以在相同硬件条件下显著提高程序的运行速度，减少资源消耗。本文将深入探讨算法优化的各种技术和方法，从理论分析到实际应用，帮助开发者全面提升代码性能。\u003c/p\u003e","title":"算法优化：提升代码性能的实用技巧"},{"content":"图像处理基础：从像素到理解 图像处理是计算机视觉领域的基础，它涉及对图像进行各种操作以提取有用信息、增强图像质量或准备图像进行进一步分析。本文将介绍图像处理的基本概念、常用技术和应用场景。\n图像的基本表示 像素与图像矩阵 在数字世界中，图像由像素（Picture Element，简称Pixel）组成。每个像素代表图像中的一个点，具有特定的位置和值。对于灰度图像，每个像素的值表示亮度，通常范围是0（黑色）到255（白色）。对于彩色图像，通常使用RGB（红、绿、蓝）三个通道表示，每个通道的值范围也是0到255。\n在计算机中，图像通常表示为矩阵。灰度图像是二维矩阵，而彩色图像是三维矩阵（高度×宽度×通道数）。\n1 2 3 4 5 6 7 8 # Python中使用NumPy表示图像 import numpy as np # 创建一个100x100的灰度图像（全黑） gray_image = np.zeros((100, 100), dtype=np.uint8) # 创建一个100x100x3的彩色图像（全黑） color_image = np.zeros((100, 100, 3), dtype=np.uint8) 图像类型 二值图像：每个像素只有两个可能的值（通常是0和1），表示黑白两色。 灰度图像：每个像素有一个值，表示从黑到白的灰度级别。 彩色图像：每个像素有多个值，通常使用RGB、HSV或CMYK等颜色模型表示。 多光谱图像：包含多个光谱通道的图像，如卫星图像。 3D图像：表示三维空间数据的图像，如医学CT扫描。 基本图像操作 图像读取与显示 使用Python的OpenCV库可以轻松读取和显示图像：\n1 2 3 4 5 6 7 8 9 10 11 12 13 import cv2 import matplotlib.pyplot as plt # 读取图像 image = cv2.imread(\u0026#39;image.jpg\u0026#39;) # 转换颜色空间（OpenCV默认使用BGR，而matplotlib使用RGB） image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 显示图像 plt.imshow(image_rgb) plt.axis(\u0026#39;off\u0026#39;) # 不显示坐标轴 plt.show() 图像缩放与旋转 1 2 3 4 5 6 7 8 # 缩放图像 resized_image = cv2.resize(image, (width, height)) # 旋转图像 (h, w) = image.shape[:2] center = (w // 2, h // 2) M = cv2.getRotationMatrix2D(center, 45, 1.0) # 旋转45度，缩放因子为1.0 rotated_image = cv2.warpAffine(image, M, (w, h)) 图像裁剪与拼接 1 2 3 4 5 6 7 8 # 裁剪图像 (y1:y2, x1:x2) cropped_image = image[100:400, 200:500] # 拼接图像 (水平拼接) horizontal_concat = np.hstack((image1, image2)) # 垂直拼接 vertical_concat = np.vstack((image1, image2)) 图像增强技术 亮度与对比度调整 1 2 3 4 5 # 亮度调整 (增加50个单位) brightness_image = cv2.add(image, np.ones(image.shape, dtype=np.uint8) * 50) # 对比度调整 (1.5倍) contrast_image = cv2.multiply(image, 1.5) 直方图均衡化 直方图均衡化是一种增强图像对比度的方法，通过重新分布图像的像素强度，使其直方图平坦化。\n1 2 3 # 灰度图像直方图均衡化 gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) equalized_image = cv2.equalizeHist(gray_image) 伽马校正 伽马校正用于调整图像的亮度，特别适用于显示设备的非线性响应。\ngamma_image = gamma_correction(image, 2.2) # 典型的伽马值\n1 2 3 4 5 6 7 8 9 # 伽马校正函数 def gamma_correction(image, gamma=1.0): # 构建查找表 inv_gamma = 1.0 / gamma table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\u0026#34;uint8\u0026#34;) # 应用伽马校正 return cv2.LUT(image, table) gamma_image = gamma_correction(image, 2.2) # 典型的伽马值 图像滤波 图像滤波是图像处理中的基本操作，用于去噪、边缘检测和特征提取等任务。\n均值滤波 均值滤波是最简单的滤波方法之一，它用邻域像素的平均值替换中心像素。\n1 2 # 5x5均值滤波 blurred_image = cv2.blur(image, (5, 5)) 高斯滤波 高斯滤波使用高斯函数作为权重，对邻域像素进行加权平均，能够有效减少噪声同时保留边缘信息。\n1 2 # 5x5高斯滤波 gaussian_blurred = cv2.GaussianBlur(image, (5, 5), 0) 中值滤波 中值滤波用邻域像素的中值替换中心像素，对于去除椒盐噪声特别有效。\n1 2 # 5x5中值滤波 median_blurred = cv2.medianBlur(image, 5) 双边滤波 双边滤波在考虑空间邻近度的同时，也考虑像素值的相似性，能够在平滑图像的同时保留边缘。\n1 2 # 双边滤波 bilateral_filtered = cv2.bilateralFilter(image, 9, 75, 75) 边缘检测 边缘检测是图像处理中的重要任务，用于识别图像中的物体边界。\nSobel算子 1 2 3 4 5 6 7 8 9 10 11 12 # 转换为灰度图像 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Sobel边缘检测 sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3) # 水平方向 sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3) # 垂直方向 # 计算梯度幅值 gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2) # 归一化到0-255范围 gradient_magnitude = np.uint8(gradient_magnitude / gradient_magnitude.max() * 255) Canny边缘检测 Canny边缘检测是一种多阶段的边缘检测算法，被认为是目前最优的边缘检测方法之一。\n1 2 # Canny边缘检测 edges = cv2.Canny(gray, 100, 200) # 阈值1和阈值2 Laplacian算子 1 2 3 # Laplacian边缘检测 laplacian = cv2.Laplacian(gray, cv2.CV_64F) laplacian = np.uint8(np.absolute(laplacian)) 形态学操作 形态学操作基于图像的形状，常用于二值图像的处理。\n腐蚀与膨胀 1 2 3 4 5 6 7 8 # 创建一个5x5的核 kernel = np.ones((5, 5), np.uint8) # 腐蚀操作 eroded_image = cv2.erode(binary_image, kernel, iterations=1) # 膨胀操作 dilated_image = cv2.dilate(binary_image, kernel, iterations=1) 开运算与闭运算 1 2 3 4 5 # 开运算（先腐蚀后膨胀） opening = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel) # 闭运算（先膨胀后腐蚀） closing = cv2.morphologyEx(binary_image, cv2.MORPH_CLOSE, kernel) 形态学梯度 1 2 # 形态学梯度（膨胀减腐蚀） gradient = cv2.morphologyEx(binary_image, cv2.MORPH_GRADIENT, kernel) 图像分割 图像分割是将图像划分为多个区域或对象的过程，是计算机视觉中的重要任务。\n阈值分割 1 2 3 4 5 6 # 全局阈值分割 _, thresholded = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY) # 自适应阈值分割 adaptive_threshold = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2) 分水岭算法 分水岭算法是一种基于拓扑理论的图像分割方法，特别适用于对接触物体的分割。\n1 2 3 4 5 6 7 8 # 标记背景和前景 ret, markers = cv2.connectedComponents(sure_foreground) markers = markers + 1 markers[unknown == 255] = 0 # 应用分水岭算法 markers = cv2.watershed(image, markers) image[markers == -1] = [255, 0, 0] # 标记分水岭边界 K-means聚类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 将图像重塑为2D数组 pixel_values = image.reshape((-1, 3)) pixel_values = np.float32(pixel_values) # 定义停止标准和K值 criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2) k = 3 # 应用K-means聚类 _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS) # 转换回原始图像形状并应用聚类结果 centers = np.uint8(centers) segmented_image = centers[labels.flatten()] segmented_image = segmented_image.reshape(image.shape) 图像特征提取 特征提取是从图像中提取有意义信息的过程，这些信息可以用于图像识别、分类和检索等任务。\n角点检测 1 2 3 4 5 6 7 # Harris角点检测 gray = np.float32(gray) harris_corners = cv2.cornerHarris(gray, 2, 3, 0.04) harris_corners = cv2.dilate(harris_corners, None) # 标记角点 image[harris_corners \u0026gt; 0.01 * harris_corners.max()] = [0, 0, 255] SIFT特征 SIFT（Scale-Invariant Feature Transform）是一种用于检测和描述图像局部特征的算法。\n1 2 3 4 5 6 7 8 # 创建SIFT对象 sift = cv2.SIFT_create() # 检测关键点和计算描述符 keypoints, descriptors = sift.detectAndCompute(gray, None) # 绘制关键点 sift_image = cv2.drawKeypoints(gray, keypoints, None) ORB特征 ORB是一种快速的特征检测器和描述符，结合了FAST关键点检测器和BRIEF描述符。\n1 2 3 4 5 6 7 8 # 创建ORB对象 orb = cv2.ORB_create() # 检测关键点和计算描述符 keypoints, descriptors = orb.detectAndCompute(gray, None) # 绘制关键点 orb_image = cv2.drawKeypoints(gray, keypoints, None) 应用场景 图像处理技术广泛应用于各个领域：\n医学影像：CT、MRI图像的分析和诊断，细胞计数，病变检测等。 自动驾驶：车道线检测，障碍物识别，交通标志识别等。 安防监控：人脸识别，行为分析，异常检测等。 工业检测：产品缺陷检测，尺寸测量，质量控制等。 遥感图像：土地利用分类，环境监测，灾害评估等。 增强现实：图像配准，目标跟踪，场景理解等。 数字娱乐：图像美化，特效处理，虚拟现实等。 总结 图像处理是计算机视觉的基础，涵盖了从基本的像素操作到复杂的特征提取和分析。本文介绍了图像的基本表示、基本操作、图像增强技术、滤波方法、边缘检测、形态学操作、图像分割和特征提取等内容。\n掌握这些基础知识对于进一步学习计算机视觉和深度学习至关重要。在实际应用中，通常需要根据具体问题选择合适的图像处理方法，并可能需要组合多种技术来达到最佳效果。\n随着深度学习技术的发展，许多传统的图像处理任务现在也可以通过深度学习方法实现，但理解传统图像处理的基本原理仍然非常重要，这有助于我们更好地理解和应用深度学习模型。\n希望本文能够帮助你入门图像处理领域，为后续的学习和研究打下坚实的基础。\n","permalink":"http://localhost:13141/posts/image-processing-basics/","summary":"\u003ch1 id=\"图像处理基础从像素到理解\"\u003e图像处理基础：从像素到理解\u003c/h1\u003e\n\u003cp\u003e图像处理是计算机视觉领域的基础，它涉及对图像进行各种操作以提取有用信息、增强图像质量或准备图像进行进一步分析。本文将介绍图像处理的基本概念、常用技术和应用场景。\u003c/p\u003e","title":"图像处理基础：从像素到滤波"},{"content":"404 - 页面不存在 抱歉，您访问的页面不存在。\n您可以尝试： 返回首页 查看文章列表 使用搜索功能 查看网站地图 如果问题仍然存在，请通过关于页面中的联系方式与我联系。\n","permalink":"http://localhost:13141/404/","summary":"\u003ch1 id=\"404---页面不存在\"\u003e404 - 页面不存在\u003c/h1\u003e\n\u003cp\u003e抱歉，您访问的页面不存在。\u003c/p\u003e\n\u003ch2 id=\"您可以尝试\"\u003e您可以尝试：\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/\"\u003e返回首页\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/\"\u003e查看文章列表\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/search/\"\u003e使用搜索功能\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sitemap.xml\"\u003e查看网站地图\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如果问题仍然存在，请通过\u003ca href=\"/about/\"\u003e关于页面\u003c/a\u003e中的联系方式与我联系。\u003c/p\u003e","title":"404 页面不存在"}]